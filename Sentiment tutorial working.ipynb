{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis\n",
    "\n",
    "* Sentiment analysis aims to determine the attitude of a speaker or a writer with respect to some topic or the overall contextual polarity of a document.\n",
    "\n",
    "* Most common classes: positive, negative, and neutral.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset: IMDB Movie reviews sentiment classification\n",
    "\n",
    "* Dataset of 25,000 movies reviews from IMDB, labeled by sentiment (positive/negative). \n",
    "* Reviews have been preprocessed, and each review is encoded as a sequence of word indexes (integers). \n",
    "* Words are indexed by overall frequency in the dataset\n",
    "\n",
    "[Source: https://keras.io/datasets/#imdb-movie-reviews-sentiment-classification]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000,)\n",
      "(25000,)\n",
      "[1, 13, 21, 15, 42, 529, 972, 1621, 1384, 64]\n",
      "[1, 590, 201, 13, 30, 5, 716, 9, 9, 2]\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import imdb\n",
    "\n",
    "# Loading the IMBD dataset\n",
    "# Selecting the 2000 most frequent words\n",
    "(x_train_org, y_train), (x_test_org, y_test) = imdb.load_data(path=\"imdb.npz\",\n",
    "                                                      num_words=2000,\n",
    "                                                      skip_top=0,\n",
    "                                                      maxlen=None,\n",
    "                                                      seed=113,\n",
    "                                                      start_char=1,\n",
    "                                                      oov_char=2,\n",
    "                                                      index_from=2)\n",
    "\n",
    "print(x_train_org.shape)\n",
    "print(x_test_org.shape)\n",
    "\n",
    "print(x_train_org[0][:10])\n",
    "print(x_test_org[0][:10])\n",
    "\n",
    "# Reducing the data size to be run on CPUs\n",
    "x_train_org = x_train_org[:5000]\n",
    "y_train = y_train[:5000]\n",
    "x_test_org = x_test_org[:500]\n",
    "y_test= y_test[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000,)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_org.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words: 88584\n"
     ]
    }
   ],
   "source": [
    "# Loading the vocabulary\n",
    "import numpy as np\n",
    "\n",
    "vocab = imdb.get_word_index(path=\"./imdb_word_index.json\")\n",
    "print(\"Number of unique words: %d\" % len(vocab))\n",
    "\n",
    "INDEX_FROM = 2 \n",
    "\n",
    "# Dict {word:id}\n",
    "word_to_id = {x:vocab[x]+INDEX_FROM for x in vocab if vocab[x]<=2000}\n",
    "word_to_id[\"<START>\"] = 1\n",
    "word_to_id[\"<UNK>\"] = 2\n",
    "\n",
    "\n",
    "# Dict {id:word}\n",
    "id_to_word = {word_to_id[x]:x for x in word_to_id}\n",
    "\n",
    "# Array of ordered words by their frequency + special characters\n",
    "vocab_list = np.array([\"<PAD>\"]+[id_to_word[x] for x in range(1,2001)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2001"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_list.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: [0 1]\n",
      "Review: <START> and that's why hard to rate br br from the adult point of view <UNK> student point of view i must say i fell nearly <UNK> here sure there is some laughing scene all the credit takes here eddie but that can't save the disney type of script and whole movie that's why br br 2 out of 10\n",
      "negative\n"
     ]
    }
   ],
   "source": [
    "# Summarize number of classes\n",
    "print(\"Classes: %s\"% np.unique(y_train))\n",
    "# y = {0:negative | 1:positive}\n",
    "\n",
    "i = 500\n",
    "print(\"Review: %s\" % \" \".join(vocab_list[np.array(x_train_org[i])]))\n",
    "print(\"Class: %s\" % \"positive\" if y_train[i] == 1 else \"negative\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_W(word_vecs, k=300):\n",
    "    \"\"\"\n",
    "    Get word matrix. W[i] is the vector for word indexed by i\n",
    "    \"\"\"\n",
    "    vocab_size = len(word_vecs)\n",
    "    word_idx_map = dict()\n",
    "    W = np.zeros(shape=(vocab_size + 1, k), dtype='float32')\n",
    "    W[0] = np.zeros(k, dtype='float32')\n",
    "    i = 1\n",
    "    for word in word_vecs:\n",
    "        W[i] = word_vecs[word]\n",
    "        word_idx_map[word] = i\n",
    "        i += 1\n",
    "    return W, word_idx_map\n",
    "\n",
    "\n",
    "def load_bin_vec(fname, vocab):\n",
    "    \"\"\"\n",
    "    Loads 300x1 word vecs from Google (Mikolov) word2vec\n",
    "    \"\"\"\n",
    "    word_vecs = {}\n",
    "    with open(fname, \"rb\") as f:\n",
    "        header = f.readline()\n",
    "        # ~ print(header)\n",
    "        vocab_size, layer1_size = map(int, header.split())\n",
    "        binary_len = np.dtype('float32').itemsize * layer1_size\n",
    "        # print(vocab_size)\n",
    "        for line in range(vocab_size):\n",
    "            # print(line)\n",
    "            word = []\n",
    "            while True:\n",
    "                ch = f.read(1).decode('iso-8859-1')\n",
    "                if ch == ' ':\n",
    "                    word = ''.join(word)\n",
    "                    break\n",
    "                if ch != '\\n':\n",
    "                    word.append(ch)\n",
    "            # print(word)\n",
    "            if word in vocab:\n",
    "                # print(word)\n",
    "                word_vecs[word] = np.frombuffer(f.read(binary_len), dtype='float32')\n",
    "            else:\n",
    "                f.read(binary_len)\n",
    "\n",
    "    return word_vecs\n",
    "\n",
    "def add_unknown_words(word_vecs, vocab, min_df=1, k=300):\n",
    "    \"\"\"\n",
    "    For words that occur in at least min_df documents, create a separate word vector.    \n",
    "    0.25 is chosen so the unknown vectors have (approximately) same variance as pre-trained ones\n",
    "    \"\"\"\n",
    "    for word in vocab:\n",
    "        if word not in word_vecs and vocab[word] >= min_df:\n",
    "            word_vecs[word] = np.random.uniform(-0.25, 0.25, k)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num words found: 1966\n",
      "W shape: (2003, 300)\n",
      "59 seconds to get the embeddings\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "w2v_file = \"./GoogleNews-vectors-negative300.bin\"\n",
    "w2v = load_bin_vec(w2v_file, word_to_id)\n",
    "print(\"num words found: %d\" % len(w2v))\n",
    "add_unknown_words(w2v, word_to_id, k=300)\n",
    "W, word_idx_map = get_W(w2v, k=300)\n",
    "\n",
    "print(\"W shape: %s\" % str(W.shape))\n",
    "\n",
    "print(\"%d seconds to get the embeddings\" % (time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max 1851\n",
      "mean 243\n",
      "[218, 189, 141, 550, 147, 43, 123, 562, 233, 130, 450, 99, 117, 238, 109, 129, 163, 752, 212, 177, 129, 140, 256, 888, 93, 142, 220, 193, 171, 221, 174, 647, 233, 162, 597, 234, 51, 336, 139, 231, 704, 142, 861, 132, 122, 570, 55, 214, 103, 186, 113, 169, 469, 138, 302, 766, 351, 146, 59, 206, 107, 152, 186, 431, 147, 684, 383, 324, 252, 263, 787, 211, 314, 118, 390, 132, 710, 306, 167, 115, 95, 158, 156, 82, 502, 314, 190, 174, 60, 145, 214, 659, 408, 515, 461, 202, 238, 170, 107, 171, 158, 145, 790, 258, 287, 67, 123, 975, 775, 236, 195, 274, 214, 91, 1038, 815, 183, 206, 50, 118, 147, 141, 60, 56, 439, 439, 213, 144, 533, 303, 203, 563, 129, 153, 55, 92, 174, 187, 183, 165, 78, 198, 156, 223, 127, 61, 362, 84, 57, 176, 159, 57, 159, 165, 213, 194, 149, 130, 203, 19, 98, 466, 525, 130, 322, 153, 408, 215, 472, 143, 136, 354, 260, 319, 125, 209, 282, 810, 142, 240, 148, 198, 193, 123, 128, 103, 479, 345, 263, 165, 205, 333, 184, 92, 177, 335, 120, 121, 259, 180, 160, 114, 59, 343, 513, 133, 206, 152, 206, 572, 153, 139, 151, 129, 129, 196, 433, 199, 140, 311, 151, 200, 584, 127, 513, 781, 932, 526, 161, 646, 135, 52, 267, 174, 185, 219, 81, 219, 131, 153, 270, 644, 155, 546, 284, 85, 293, 155, 358, 45, 231, 124, 178, 118, 260, 393, 127, 157, 107, 322, 188, 126, 155, 294, 249, 177, 138, 215, 263, 132, 150, 217, 188, 385, 199, 127, 325, 161, 140, 215, 240, 230, 327, 129, 113, 225, 87, 496, 234, 311, 215, 111, 102, 110, 165, 839, 296, 130, 104, 274, 229, 235, 653, 468, 578, 139, 315, 65, 178, 836, 164, 239, 212, 297, 258, 157, 78, 544, 152, 120, 208, 163, 226, 304, 195, 454, 121, 175, 617, 320, 121, 245, 655, 114, 131, 104, 238, 138, 164, 52, 215, 87, 471, 142, 289, 106, 141, 239, 412, 154, 175, 828, 41, 144, 525, 176, 551, 251, 621, 159, 75, 207, 80, 94, 78, 449, 622, 157, 85, 260, 1011, 444, 326, 586, 118, 270, 360, 95, 640, 315, 138, 573, 434, 313, 128, 1007, 130, 257, 209, 159, 602, 109, 250, 117, 149, 199, 55, 578, 158, 323, 486, 177, 73, 127, 138, 130, 110, 283, 244, 375, 137, 156, 153, 112, 94, 137, 195, 137, 112, 25, 106, 105, 272, 857, 116, 58, 114, 71, 57, 292, 56, 157, 283, 87, 327, 481, 918, 129, 181, 214, 601, 215, 117, 112, 401, 165, 154, 587, 417, 345, 233, 496, 403, 317, 189, 340, 195, 370, 194, 150, 559, 48, 129, 110, 45, 173, 674, 416, 233, 146, 73, 146, 190, 130, 127, 217, 785, 227, 119, 149, 150, 175, 588, 130, 414, 172, 523, 119, 130, 122, 219, 264, 202, 218, 367, 230, 429, 985, 144, 128, 60, 78, 125, 131, 186, 283, 121, 226, 82, 57, 468, 336, 218, 110, 535, 603, 147, 117, 156, 72, 72, 69, 529, 148, 56, 263, 202, 348, 172, 104, 212, 146, 191, 251, 179, 722, 156, 141, 235, 97, 69, 222, 228, 233, 46, 130, 599, 146, 71, 144, 132, 89, 115, 267, 100, 148, 197, 627, 161, 263, 447, 97, 132, 357, 52, 329, 149, 253, 330, 549, 166, 190, 165, 202, 351, 633, 942, 123, 121, 184, 270, 139, 248, 382, 292, 215, 439, 177, 42, 173, 173, 233, 480, 377, 48, 177, 192, 84, 176, 150, 467, 210, 687, 631, 279, 136, 67, 167, 170, 359, 451, 135, 197, 1009, 87, 241, 492, 336, 88, 170, 425, 459, 331, 199, 158, 69, 121, 116, 120, 297, 173, 293, 103, 477, 723, 133, 413, 109, 101, 227, 141, 939, 199, 162, 377, 172, 532, 68, 41, 130, 83, 136, 787, 117, 165, 111, 124, 552, 172, 130, 96, 153, 181, 49, 119, 88, 100, 297, 363, 632, 218, 168, 100, 212, 120, 84, 135, 268, 127, 746, 127, 236, 335, 193, 843, 251, 165, 210, 239, 311, 214, 29, 334, 168, 127, 169, 110, 164, 147, 205, 133, 188, 125, 183, 326, 150, 290, 214, 328, 51, 343, 212, 454, 96, 471, 82, 523, 114, 147, 200, 387, 174, 150, 103, 187, 183, 115, 144, 131, 241, 57, 46, 23, 419, 227, 110, 91, 66, 137, 104, 164, 417, 127, 78, 181, 179, 181, 261, 243, 205, 155, 166, 180, 77, 440, 284, 328, 258, 61, 193, 39, 156, 124, 108, 322, 376, 162, 359, 139, 302, 646, 44, 253, 244, 438, 432, 226, 236, 336, 166, 444, 195, 74, 127, 213, 195, 148, 174, 148, 149, 136, 77, 53, 151, 215, 129, 196, 135, 146, 235, 166, 359, 132, 167, 141, 188, 365, 138, 118, 110, 309, 261, 127, 76, 900, 211, 273, 233, 149, 212, 302, 268, 295, 212, 143, 467, 256, 230, 129, 145, 119, 83, 322, 208, 132, 149, 278, 136, 198, 164, 349, 200, 278, 439, 130, 161, 223, 561, 397, 302, 282, 187, 192, 107, 361, 126, 201, 79, 415, 133, 300, 124, 355, 978, 323, 114, 85, 143, 221, 190, 215, 244, 626, 64, 127, 180, 128, 206, 221, 261, 122, 201, 221, 139, 315, 356, 254, 131, 128, 234, 129, 66, 134, 121, 470, 373, 129, 127, 92, 127, 223, 179, 651, 452, 935, 146, 170, 438, 94, 159, 312, 302, 128, 121, 280, 622, 166, 126, 157, 353, 45, 132, 324, 65, 304, 149, 243, 117, 719, 164, 116, 61, 368, 407, 73, 571, 170, 303, 159, 42, 400, 295, 183, 75, 215, 255, 229, 445, 1000, 494, 128, 580, 130, 59, 189, 526, 116, 114, 544, 118, 111, 420, 250, 258, 188, 139, 89, 135, 58, 173, 84, 150, 170, 159, 152, 170, 74, 213, 41, 250, 124, 533, 84, 186, 126, 183, 125, 153, 282, 146, 149, 215, 122, 68, 139, 128, 113, 252, 168, 434, 298, 181, 216, 152, 99, 182, 135, 169, 103, 109, 218, 62, 1022, 546, 313, 261, 248, 57, 100, 544, 290, 197, 132, 170, 151, 707, 263, 142, 164, 141, 371, 298, 324, 142, 237, 244, 40, 230, 158, 142, 379, 124, 178, 108, 149, 741, 125, 410, 128, 243, 179, 138, 610, 542, 90, 187, 172, 186, 106, 27, 237, 98, 140, 125, 245, 147, 137, 84, 264, 147, 257, 220, 82, 159, 239, 149, 157, 160, 183, 185, 253, 230, 167, 105, 218, 85, 367, 209, 66, 180, 253, 214, 219, 320, 200, 216, 112, 184, 173, 499, 196, 340, 308, 144, 240, 72, 657, 753, 164, 925, 169, 694, 395, 144, 152, 142, 596, 431, 160, 985, 143, 138, 171, 215, 110, 140, 486, 267, 344, 129, 162, 88, 116, 150, 232, 104, 39, 932, 209, 207, 344, 153, 220, 183, 310, 328, 196, 284, 232, 203, 135, 122, 271, 132, 133, 242, 593, 319, 331, 152, 160, 284, 191, 409, 198, 171, 159, 297, 164, 124, 184, 139, 356, 296, 184, 633, 656, 156, 170, 223, 98, 273, 37, 60, 166, 679, 123, 89, 168, 189, 247, 110, 633, 642, 133, 182, 134, 125, 133, 84, 111, 179, 185, 149, 324, 517, 174, 186, 716, 959, 129, 130, 147, 122, 180, 167, 142, 240, 151, 117, 196, 137, 171, 199, 135, 153, 197, 209, 253, 132, 133, 137, 232, 138, 212, 171, 790, 51, 97, 71, 118, 49, 1009, 149, 255, 229, 123, 204, 469, 201, 110, 228, 133, 467, 318, 211, 107, 132, 179, 304, 239, 114, 186, 187, 261, 137, 263, 132, 154, 159, 254, 147, 502, 202, 117, 150, 117, 204, 144, 135, 215, 478, 919, 141, 129, 125, 341, 189, 496, 107, 278, 54, 55, 270, 129, 145, 254, 278, 216, 168, 340, 193, 360, 421, 339, 138, 132, 143, 356, 137, 270, 199, 540, 240, 131, 183, 250, 131, 32, 190, 161, 71, 324, 383, 622, 364, 238, 196, 98, 76, 119, 128, 169, 148, 154, 143, 60, 178, 305, 208, 93, 197, 180, 101, 227, 204, 278, 278, 191, 429, 153, 192, 811, 208, 137, 529, 147, 67, 141, 230, 49, 137, 117, 235, 134, 128, 205, 92, 504, 135, 52, 189, 262, 96, 345, 244, 194, 702, 256, 857, 186, 200, 66, 263, 155, 52, 872, 116, 315, 536, 119, 40, 745, 222, 139, 278, 271, 730, 118, 131, 158, 121, 426, 144, 112, 266, 311, 831, 425, 83, 269, 256, 191, 166, 798, 255, 186, 167, 141, 174, 142, 73, 105, 175, 144, 130, 988, 101, 163, 107, 144, 298, 214, 118, 130, 124, 135, 558, 292, 173, 119, 282, 534, 205, 42, 99, 50, 146, 67, 206, 145, 255, 223, 425, 198, 727, 254, 204, 155, 174, 117, 160, 321, 146, 105, 123, 183, 189, 145, 209, 120, 193, 158, 163, 131, 109, 267, 209, 116, 543, 466, 134, 57, 171, 358, 172, 124, 58, 950, 323, 205, 174, 368, 290, 156, 184, 465, 58, 373, 97, 366, 140, 177, 127, 57, 295, 608, 488, 599, 338, 140, 211, 384, 281, 130, 172, 89, 129, 736, 142, 158, 357, 211, 382, 113, 121, 248, 145, 639, 96, 83, 324, 183, 559, 335, 634, 341, 198, 291, 323, 306, 99, 193, 193, 177, 81, 252, 277, 424, 155, 315, 445, 113, 68, 431, 173, 188, 125, 177, 261, 144, 439, 277, 52, 128, 254, 69, 275, 198, 277, 317, 321, 100, 173, 110, 242, 453, 679, 576, 206, 475, 272, 334, 207, 82, 289, 126, 122, 122, 59, 63, 125, 414, 131, 261, 184, 210, 127, 167, 134, 414, 141, 175, 263, 118, 154, 96, 206, 166, 151, 119, 133, 184, 430, 409, 153, 211, 228, 402, 61, 346, 185, 186, 149, 197, 141, 142, 233, 124, 201, 252, 152, 128, 297, 129, 357, 142, 252, 125, 155, 165, 306, 125, 314, 61, 189, 164, 92, 225, 240, 288, 146, 373, 543, 153, 117, 418, 190, 151, 723, 43, 52, 236, 163, 318, 170, 65, 405, 233, 144, 148, 190, 170, 201, 631, 127, 80, 74, 369, 127, 140, 133, 542, 133, 184, 136, 699, 231, 265, 214, 347, 302, 209, 59, 108, 143, 760, 147, 909, 224, 114, 134, 254, 112, 104, 319, 621, 182, 232, 166, 122, 138, 400, 365, 186, 153, 114, 151, 450, 702, 155, 230, 214, 182, 100, 469, 202, 335, 131, 358, 124, 156, 319, 689, 171, 291, 48, 261, 54, 314, 234, 156, 316, 962, 430, 211, 253, 256, 135, 365, 325, 115, 125, 171, 228, 127, 182, 105, 235, 329, 113, 174, 150, 162, 123, 218, 333, 140, 402, 81, 293, 205, 190, 148, 292, 112, 664, 411, 161, 347, 424, 160, 181, 210, 114, 359, 113, 220, 300, 248, 636, 178, 259, 178, 1015, 378, 152, 924, 475, 997, 279, 145, 148, 125, 136, 65, 107, 124, 139, 184, 135, 410, 256, 148, 122, 83, 56, 462, 126, 61, 101, 121, 271, 1010, 178, 110, 41, 159, 447, 185, 183, 90, 150, 524, 172, 184, 149, 121, 215, 82, 245, 139, 55, 184, 115, 134, 335, 627, 203, 267, 228, 173, 196, 130, 271, 179, 49, 67, 138, 112, 61, 88, 152, 64, 124, 360, 127, 240, 158, 346, 133, 872, 73, 212, 206, 141, 191, 325, 207, 155, 175, 423, 269, 128, 645, 310, 161, 110, 114, 331, 525, 125, 50, 139, 243, 168, 260, 119, 65, 795, 158, 124, 235, 116, 281, 116, 177, 838, 96, 68, 103, 189, 242, 135, 201, 137, 384, 198, 278, 89, 123, 227, 854, 129, 488, 115, 63, 80, 50, 128, 179, 285, 214, 173, 325, 147, 228, 495, 125, 223, 216, 541, 708, 42, 165, 730, 132, 128, 347, 233, 247, 446, 304, 337, 438, 130, 511, 175, 101, 198, 114, 160, 128, 91, 148, 563, 43, 159, 115, 56, 391, 149, 340, 363, 152, 187, 204, 133, 624, 136, 116, 375, 169, 156, 477, 134, 165, 138, 129, 201, 790, 165, 182, 114, 178, 124, 141, 417, 379, 55, 109, 229, 53, 166, 135, 200, 183, 169, 202, 630, 137, 80, 136, 151, 596, 624, 196, 475, 356, 308, 203, 91, 151, 127, 124, 89, 125, 487, 221, 410, 183, 299, 354, 238, 167, 161, 150, 339, 415, 152, 330, 207, 123, 212, 124, 136, 134, 350, 125, 370, 150, 438, 121, 156, 283, 132, 140, 229, 125, 113, 191, 245, 135, 751, 54, 240, 230, 148, 174, 389, 390, 1011, 146, 535, 245, 65, 160, 141, 197, 245, 166, 146, 128, 84, 110, 118, 157, 318, 50, 263, 258, 73, 120, 371, 197, 146, 226, 207, 76, 169, 426, 312, 652, 121, 138, 156, 397, 116, 57, 645, 203, 16, 353, 70, 125, 164, 258, 396, 185, 110, 148, 127, 163, 117, 439, 138, 386, 114, 37, 175, 1034, 238, 125, 52, 406, 640, 131, 526, 180, 75, 190, 182, 267, 126, 191, 697, 162, 371, 155, 276, 257, 152, 351, 80, 139, 133, 143, 75, 112, 226, 112, 186, 145, 209, 315, 120, 758, 93, 220, 128, 137, 129, 328, 648, 443, 159, 125, 39, 160, 456, 402, 48, 112, 91, 142, 125, 758, 835, 228, 188, 185, 70, 667, 128, 300, 157, 392, 229, 442, 64, 123, 49, 47, 680, 139, 92, 166, 221, 234, 724, 119, 263, 462, 159, 120, 105, 188, 115, 184, 419, 155, 292, 1002, 447, 113, 130, 240, 147, 216, 136, 150, 119, 305, 250, 64, 547, 142, 186, 269, 45, 592, 50, 167, 141, 345, 158, 162, 541, 314, 349, 148, 143, 317, 152, 89, 383, 468, 238, 131, 160, 325, 91, 122, 111, 317, 85, 213, 193, 141, 46, 161, 147, 300, 322, 149, 90, 235, 591, 334, 126, 239, 136, 153, 173, 182, 113, 150, 161, 229, 180, 395, 125, 436, 569, 302, 168, 129, 495, 406, 297, 279, 205, 197, 122, 244, 112, 466, 139, 128, 136, 184, 122, 200, 78, 122, 268, 151, 124, 162, 479, 155, 400, 201, 131, 148, 155, 343, 135, 337, 137, 95, 436, 247, 158, 179, 259, 120, 693, 144, 196, 105, 406, 804, 142, 611, 414, 269, 172, 133, 142, 645, 475, 262, 132, 167, 128, 167, 108, 504, 70, 150, 216, 129, 90, 48, 493, 459, 635, 233, 486, 301, 256, 205, 178, 120, 240, 317, 120, 157, 141, 140, 27, 538, 47, 73, 203, 124, 100, 167, 173, 83, 117, 128, 39, 997, 167, 141, 193, 127, 353, 122, 150, 735, 501, 396, 160, 160, 124, 104, 119, 50, 179, 314, 77, 195, 348, 119, 373, 172, 140, 155, 532, 247, 246, 454, 255, 483, 125, 362, 142, 117, 49, 106, 120, 169, 526, 514, 241, 124, 67, 510, 312, 105, 273, 131, 145, 822, 249, 596, 166, 151, 436, 161, 1629, 192, 139, 125, 179, 114, 265, 166, 219, 353, 130, 159, 124, 210, 321, 407, 59, 102, 260, 313, 262, 166, 124, 279, 113, 123, 76, 350, 163, 51, 152, 261, 90, 205, 518, 98, 146, 194, 106, 502, 567, 138, 188, 123, 290, 485, 507, 150, 139, 152, 354, 65, 62, 300, 139, 718, 153, 848, 130, 131, 259, 178, 631, 149, 141, 130, 319, 241, 84, 190, 242, 1036, 133, 86, 202, 173, 497, 104, 229, 483, 102, 249, 327, 436, 56, 236, 328, 454, 49, 232, 382, 475, 180, 125, 323, 350, 106, 175, 272, 227, 146, 117, 134, 123, 182, 395, 413, 103, 245, 259, 188, 207, 117, 382, 112, 244, 148, 174, 77, 222, 107, 605, 74, 295, 158, 498, 469, 117, 51, 274, 149, 61, 118, 457, 265, 209, 45, 229, 349, 595, 445, 158, 284, 111, 146, 158, 468, 54, 538, 237, 164, 237, 197, 593, 115, 111, 122, 250, 163, 224, 275, 659, 336, 201, 197, 601, 104, 185, 435, 538, 128, 167, 234, 220, 335, 398, 46, 491, 809, 163, 108, 746, 126, 175, 116, 433, 220, 823, 153, 135, 110, 52, 644, 231, 485, 53, 338, 334, 176, 173, 144, 165, 61, 226, 481, 122, 159, 164, 470, 829, 143, 105, 416, 315, 197, 408, 362, 254, 331, 68, 107, 54, 534, 282, 162, 71, 688, 133, 71, 122, 116, 178, 659, 223, 140, 303, 423, 191, 989, 333, 136, 832, 1021, 385, 139, 164, 277, 232, 999, 170, 201, 173, 147, 158, 167, 55, 116, 44, 708, 195, 214, 123, 123, 328, 402, 51, 110, 243, 223, 204, 211, 150, 240, 146, 211, 134, 370, 475, 43, 188, 149, 292, 350, 539, 156, 249, 189, 162, 155, 102, 51, 130, 168, 588, 295, 146, 265, 231, 67, 695, 193, 155, 302, 137, 422, 309, 58, 184, 133, 133, 148, 126, 436, 103, 154, 95, 127, 111, 782, 279, 362, 119, 140, 171, 820, 151, 64, 253, 161, 171, 370, 199, 120, 96, 307, 169, 145, 342, 210, 344, 193, 294, 52, 660, 56, 271, 760, 85, 129, 124, 467, 110, 495, 502, 223, 256, 47, 121, 149, 375, 118, 850, 247, 109, 132, 130, 176, 300, 200, 121, 147, 319, 727, 176, 177, 202, 187, 128, 220, 126, 161, 49, 176, 144, 1000, 218, 108, 706, 186, 151, 210, 191, 118, 142, 114, 202, 84, 132, 121, 133, 120, 694, 124, 72, 257, 297, 162, 121, 186, 133, 633, 136, 186, 34, 125, 107, 143, 143, 325, 949, 207, 41, 194, 607, 641, 497, 91, 201, 553, 174, 306, 123, 514, 238, 171, 168, 751, 518, 286, 137, 421, 457, 118, 542, 116, 69, 348, 412, 241, 352, 139, 53, 487, 356, 85, 575, 615, 157, 86, 247, 625, 696, 236, 253, 361, 123, 201, 293, 239, 167, 53, 64, 364, 103, 431, 475, 47, 352, 106, 158, 149, 162, 773, 115, 137, 101, 148, 222, 175, 227, 164, 193, 121, 147, 501, 300, 208, 276, 277, 131, 134, 371, 40, 391, 403, 74, 146, 912, 135, 266, 163, 149, 100, 522, 89, 190, 140, 428, 99, 365, 98, 525, 96, 127, 117, 160, 351, 118, 126, 247, 284, 598, 211, 130, 241, 135, 368, 323, 220, 79, 153, 177, 301, 103, 115, 139, 263, 314, 121, 140, 388, 194, 484, 162, 217, 324, 168, 154, 748, 515, 186, 287, 287, 255, 512, 331, 80, 203, 138, 192, 578, 468, 361, 117, 275, 156, 285, 138, 122, 203, 135, 153, 160, 329, 161, 229, 117, 65, 430, 143, 606, 143, 118, 232, 221, 335, 155, 206, 121, 136, 295, 221, 674, 506, 120, 166, 111, 182, 161, 116, 355, 168, 143, 292, 320, 543, 610, 160, 390, 242, 236, 56, 957, 264, 69, 180, 154, 173, 563, 173, 750, 658, 137, 303, 101, 127, 164, 135, 783, 61, 195, 287, 679, 172, 110, 154, 111, 473, 245, 484, 263, 258, 280, 186, 108, 137, 250, 73, 137, 119, 91, 344, 283, 760, 259, 415, 520, 317, 164, 317, 499, 218, 292, 154, 165, 443, 202, 129, 347, 64, 150, 138, 80, 403, 149, 116, 223, 158, 137, 161, 155, 285, 180, 124, 424, 199, 266, 275, 143, 174, 193, 660, 69, 171, 321, 232, 125, 119, 473, 158, 244, 49, 125, 47, 333, 186, 141, 203, 198, 456, 119, 280, 50, 496, 248, 309, 106, 126, 871, 226, 80, 511, 420, 592, 262, 126, 166, 217, 377, 415, 143, 63, 530, 186, 109, 192, 499, 443, 429, 262, 150, 128, 128, 205, 80, 838, 307, 184, 1030, 565, 220, 54, 127, 208, 110, 569, 112, 251, 140, 155, 187, 165, 266, 255, 135, 168, 220, 187, 785, 128, 513, 171, 136, 622, 63, 73, 267, 125, 587, 67, 138, 197, 174, 686, 141, 260, 240, 167, 133, 424, 148, 270, 366, 72, 319, 195, 119, 151, 517, 268, 120, 111, 126, 178, 48, 146, 185, 210, 254, 62, 241, 30, 532, 214, 81, 222, 94, 50, 163, 305, 343, 691, 267, 208, 125, 142, 70, 180, 255, 229, 349, 294, 125, 158, 100, 266, 171, 596, 163, 169, 467, 406, 89, 310, 125, 387, 208, 117, 488, 1004, 63, 194, 154, 435, 233, 92, 187, 127, 203, 306, 202, 567, 150, 142, 127, 199, 174, 108, 246, 640, 522, 379, 799, 157, 354, 440, 351, 403, 357, 62, 434, 167, 100, 462, 589, 707, 117, 321, 355, 203, 123, 63, 492, 181, 121, 308, 213, 618, 271, 158, 1034, 113, 163, 413, 71, 178, 241, 142, 252, 408, 192, 271, 188, 265, 164, 404, 152, 229, 1112, 136, 116, 279, 339, 166, 317, 259, 436, 188, 81, 504, 224, 126, 1050, 339, 126, 126, 263, 119, 457, 320, 171, 169, 117, 73, 77, 715, 243, 329, 158, 48, 470, 46, 137, 128, 128, 201, 274, 50, 120, 198, 551, 323, 66, 424, 105, 65, 217, 113, 147, 187, 257, 148, 237, 112, 152, 185, 71, 128, 126, 83, 171, 203, 132, 120, 324, 464, 287, 173, 206, 136, 522, 175, 77, 153, 132, 123, 128, 227, 118, 129, 211, 552, 378, 309, 58, 702, 183, 284, 207, 128, 313, 151, 168, 297, 397, 111, 741, 130, 204, 136, 404, 962, 465, 544, 152, 443, 496, 314, 165, 200, 297, 127, 251, 155, 272, 208, 409, 344, 129, 71, 85, 140, 519, 263, 123, 235, 190, 126, 255, 409, 52, 144, 131, 122, 357, 96, 199, 280, 229, 163, 263, 319, 378, 476, 279, 210, 251, 134, 124, 216, 148, 116, 181, 178, 109, 427, 65, 130, 313, 139, 173, 152, 210, 201, 398, 137, 65, 303, 107, 126, 317, 521, 849, 343, 164, 320, 361, 168, 390, 48, 702, 119, 117, 132, 388, 660, 256, 57, 204, 216, 116, 168, 114, 251, 495, 142, 326, 96, 374, 199, 163, 184, 63, 731, 269, 499, 142, 53, 86, 238, 212, 315, 254, 162, 116, 100, 101, 119, 126, 257, 137, 410, 134, 397, 239, 757, 237, 183, 352, 437, 159, 300, 177, 294, 179, 218, 231, 149, 336, 135, 318, 114, 125, 163, 223, 397, 553, 332, 55, 270, 343, 175, 343, 138, 361, 131, 215, 265, 171, 494, 182, 497, 59, 71, 850, 512, 178, 740, 598, 62, 195, 168, 137, 270, 219, 183, 348, 203, 57, 178, 222, 257, 170, 152, 128, 142, 87, 118, 51, 355, 134, 316, 208, 220, 58, 181, 159, 63, 89, 172, 235, 462, 605, 128, 171, 171, 132, 184, 116, 67, 623, 374, 683, 155, 165, 426, 117, 612, 156, 144, 390, 227, 134, 484, 189, 139, 131, 44, 191, 180, 540, 138, 247, 237, 104, 130, 327, 233, 125, 149, 133, 158, 190, 160, 144, 246, 497, 249, 123, 173, 45, 342, 158, 168, 140, 133, 31, 287, 146, 613, 318, 154, 390, 146, 551, 349, 153, 168, 225, 452, 191, 219, 35, 60, 412, 125, 237, 140, 131, 116, 94, 124, 280, 125, 285, 201, 594, 142, 98, 130, 101, 256, 105, 146, 110, 244, 144, 132, 169, 341, 444, 127, 470, 338, 294, 69, 248, 246, 142, 133, 126, 88, 137, 143, 171, 438, 252, 136, 69, 45, 155, 955, 234, 132, 68, 767, 287, 119, 178, 184, 86, 113, 642, 139, 47, 314, 307, 196, 184, 386, 110, 299, 152, 164, 126, 174, 126, 136, 206, 56, 156, 231, 105, 242, 108, 882, 109, 180, 214, 323, 148, 123, 190, 176, 494, 326, 132, 78, 194, 294, 262, 219, 159, 138, 133, 209, 166, 57, 365, 110, 193, 112, 344, 45, 118, 351, 270, 69, 44, 102, 47, 310, 130, 128, 69, 351, 119, 607, 597, 112, 148, 173, 118, 424, 141, 142, 254, 287, 298, 399, 387, 310, 52, 246, 367, 242, 256, 987, 135, 256, 146, 285, 109, 270, 161, 231, 210, 89, 236, 229, 460, 162, 498, 233, 71, 142, 435, 184, 442, 190, 387, 85, 389, 52, 102, 174, 355, 133, 125, 365, 609, 308, 864, 654, 108, 301, 103, 223, 169, 355, 493, 97, 51, 186, 103, 153, 142, 89, 883, 155, 121, 491, 763, 133, 65, 192, 169, 171, 126, 215, 215, 138, 184, 650, 180, 194, 309, 148, 276, 1851, 502, 57, 464, 45, 234, 172, 139, 318, 205, 158, 167, 236, 352, 277, 916, 105, 179, 141, 183, 261, 281, 140, 413, 157, 200, 360, 471, 306, 590, 104, 485, 163, 136, 59, 99, 219, 148, 686, 311, 161, 151, 73, 122, 317, 235, 127, 168, 207, 987, 187, 285, 372, 243, 197, 271, 138, 154, 407, 190, 149, 128, 130, 300, 152, 136, 119, 194, 137, 701, 119, 193, 173, 130, 230, 136, 114, 162, 116, 240, 145, 382, 290, 129, 128, 122, 216, 232, 300, 137, 172, 508, 151, 124, 383, 364, 171, 504, 135, 210, 172, 726, 161, 130, 138, 141, 440, 306, 113, 123, 164, 161, 154, 253, 161, 47, 118, 124, 209, 275, 134, 67, 80, 123, 153, 208, 171, 270, 196, 107, 134, 130, 129, 149, 101, 150, 404, 146, 148, 599, 130, 194, 548, 247, 366, 764, 119, 123, 225, 217, 78, 354, 122, 397, 250, 128, 132, 312, 192, 467, 108, 115, 144, 156, 310, 241, 92, 216, 407, 131, 154, 119, 144, 205, 475, 205, 104, 715, 230, 104, 317, 41, 210, 565, 154, 62, 146, 383, 301, 349, 134, 239, 230, 223, 154, 172, 198, 141, 385, 279, 250, 200, 117, 385, 145, 396, 895, 172, 495, 148, 216, 262, 68, 481, 191, 217, 197, 177, 693, 70, 272, 134, 57, 307, 198, 142, 679, 99, 155, 171, 318, 167, 201, 173, 186, 256, 129, 345, 353, 141, 118, 591, 537, 166, 201, 117, 213, 177, 57, 496, 75, 135, 94, 108, 130, 227, 160, 121, 146, 329, 87, 180, 119, 219, 255, 349, 292, 584, 115, 64, 183, 103, 275, 371, 117, 124, 122, 102, 539, 111, 370, 299, 270, 356, 155, 141, 203, 167, 103, 189, 62, 358, 83, 84, 124, 230, 141, 205, 147, 49, 69, 318, 146, 134, 136, 118, 183, 604, 152, 424, 236, 174, 191, 214, 64, 405, 137, 686, 268, 817, 148, 65, 306, 211, 183, 55, 112, 285, 153, 293, 77, 347, 557, 62, 223, 876, 412, 139, 138, 203, 123, 123, 330, 926, 170, 530, 62, 83, 237, 660, 280, 124, 174, 158, 180, 117, 148, 733, 381, 412, 138, 101, 205, 228, 311, 156, 137, 83, 190, 521, 68, 327, 93, 341, 119, 118, 110, 275, 165, 386, 123, 492, 138, 550, 243, 291, 372, 133, 140, 94, 183, 410, 280, 352, 150, 333, 128, 71, 106, 228, 238, 115, 404, 63, 182, 296, 317, 129, 152, 89, 104, 111, 542, 136, 209, 114, 158, 440, 487, 118, 191, 187, 262, 284, 219, 116, 152, 312, 91, 145, 126, 804, 269, 174, 147, 158, 166, 250, 116, 260, 192, 157, 159, 204, 217, 96, 138, 128, 196, 184, 338, 554, 765, 75, 122, 321, 318, 581, 400, 112, 142, 135, 295, 166, 378, 236, 356, 178, 322, 40, 134, 146, 785, 655, 254, 374, 337, 137, 583, 411, 591, 145, 370, 171, 848, 177, 122, 143, 319, 159, 156, 132, 147, 268, 320, 145, 161, 120, 323, 84, 85, 146, 648, 554, 311, 162, 535, 398, 360, 104, 197, 112, 446, 244, 91, 443, 189, 128, 252, 274, 431, 140, 187, 285, 437, 118, 128, 40, 213, 274, 1014, 153, 275, 135, 114, 114, 142, 229, 136, 374, 328, 118, 285, 153, 87, 124, 176, 130, 243, 148, 189, 108, 222, 99, 247, 136, 108, 217, 416, 143, 432, 132, 868, 123, 128, 103, 69, 551, 279, 107, 363, 127, 124, 168, 679, 78, 241, 220, 69, 536, 218, 311, 206, 317, 133, 87, 132, 125, 453, 393, 79, 154, 96, 486, 43, 237, 115, 593, 119, 134, 193, 321, 298, 118, 428, 167, 354, 424, 108, 159, 185, 173, 102, 454, 497, 87, 35, 340, 856, 135, 133, 135, 177, 397, 206, 178, 105, 643, 206, 145, 160, 225, 201, 729, 125, 216, 160, 195, 695, 127, 99, 219, 150, 190, 305, 337, 179, 245, 162, 96, 134, 309, 820, 536, 210, 162, 96, 130, 261, 124, 310, 178, 121, 67, 184, 252, 130, 235, 137, 127, 126, 164, 55, 115, 46, 352, 136, 267, 124, 62, 198, 106, 198, 227, 201, 150, 802, 258, 368, 146, 118, 29, 527, 160, 45, 338, 290, 446, 482, 172, 117, 740, 231, 362, 185, 253, 143, 146, 129, 279, 61, 460, 129, 315, 240, 122, 169, 287, 173, 112, 313, 237, 209, 159, 20, 125, 116, 124, 197, 238, 84, 74, 149, 189, 283, 288, 127, 375, 113, 148, 208, 185, 46, 131, 193, 184, 478, 125, 140, 127, 180, 114, 161, 149, 127, 141, 233, 211, 240, 135, 233, 374, 183, 136, 185, 538, 80, 271, 113, 192, 84, 171, 700, 267, 126, 297, 196, 247, 239, 53, 207, 184, 868, 116, 129, 181, 463, 218, 324, 435, 546, 169, 142, 148, 118, 146, 364, 181, 131, 164, 285, 315, 214, 382, 758, 154, 119, 417, 201, 220, 147, 240, 69, 238, 229, 133, 211, 158, 562, 121, 121, 168, 459, 171, 227, 171, 137, 207, 163, 557, 168, 380, 148, 231, 286, 54, 402, 204, 120, 123, 260, 204, 203, 132, 303, 243, 321, 73, 189, 324, 244, 443, 256, 183, 132, 184, 221, 328, 141, 193, 129, 258, 122, 101, 251, 132, 439, 200, 146, 373, 175, 131, 311, 223, 431, 215, 377, 360, 640, 266, 340, 140, 199, 134, 146, 155, 349, 164, 152, 212, 202, 129, 307, 198, 196, 153, 106, 122, 142, 120, 95, 973, 169, 301, 161, 110, 629, 399, 264, 790, 506, 147, 131, 252, 122, 287, 147, 209, 152, 325, 186, 332, 167, 165, 284, 182, 273, 334, 244, 320, 448, 721, 142, 266, 623, 191, 209, 138, 177, 115, 79, 322, 269, 213, 56, 183, 285, 151, 358, 206, 51, 394, 37, 134, 141, 144, 106, 321, 216, 519, 144, 221, 532, 131, 643, 356, 137, 137, 204, 209, 146, 96, 194, 121, 155, 155, 150, 129, 461, 244, 199, 199, 92, 131, 329, 129, 121, 150, 166, 252, 198, 405, 382, 102, 133, 138, 727, 109, 134, 152, 456, 517, 175, 191, 387, 490, 143, 833, 208, 182, 324, 127, 131, 96, 171, 146, 1020, 135, 261, 179, 824, 135, 140, 163, 321, 852, 130, 171]\n"
     ]
    }
   ],
   "source": [
    "# Max and avg number of word\n",
    "lengths = [len(x) for x in x_train_org]\n",
    "print(\"max %d\" % max(lengths))\n",
    "print(\"mean %d\" % np.mean(lengths))\n",
    "print(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   1   13   21 ...,    0    0    0]\n",
      " [   1  193 1152 ...,    0    0    0]\n",
      " [   1   13   46 ...,    0    0    0]\n",
      " ..., \n",
      " [   1   13   15 ...,   20    2  599]\n",
      " [   1   12  331 ...,    0    0    0]\n",
      " [   1  206  114 ...,    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "# Padding the input data\n",
    "from keras.preprocessing import sequence\n",
    "input_length = 350 # average length \n",
    "\n",
    "x_train = sequence.pad_sequences(x_train_org, maxlen=input_length, padding='post', truncating='post')\n",
    "x_test = sequence.pad_sequences(x_test_org, maxlen=input_length, padding='post', truncating='post')\n",
    "print(x_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "abc=[1,2,3,4,5]\n",
    "input_length= 10\n",
    "def pad(ut):\n",
    "    return ut[:10]+['' for i in range(input_length-len(ut))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, '', '', '', '', '']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad(abc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two dense layers for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 350, 300)          600900    \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 105000)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                1050010   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 1,651,031\n",
      "Trainable params: 1,651,031\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# create the model - Two dense layer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(W.shape[0], W.shape[1], input_length=input_length, weights=[W]))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "print(model.summary())\n",
    "# plot_model(model, to_file='Sent_FF.png', show_shapes=False, show_layer_names=True, rankdir='TB')\n",
    "\n",
    "# loss function = binary_crossentropy\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5000 samples, validate on 500 samples\n",
      "Epoch 1/5\n",
      " - 11s - loss: 0.6972 - acc: 0.5064 - val_loss: 0.6936 - val_acc: 0.4760\n",
      "Epoch 2/5\n",
      " - 7s - loss: 0.6928 - acc: 0.5082 - val_loss: 0.6946 - val_acc: 0.4760\n",
      "Epoch 3/5\n",
      " - 6s - loss: 0.6802 - acc: 0.5202 - val_loss: 0.6882 - val_acc: 0.5680\n",
      "Epoch 4/5\n",
      " - 5s - loss: 0.5131 - acc: 0.7572 - val_loss: 0.6739 - val_acc: 0.6380\n",
      "Epoch 5/5\n",
      " - 6s - loss: 0.1444 - acc: 0.9584 - val_loss: 0.6478 - val_acc: 0.7100\n",
      "Accuracy: 71.00%\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=5, batch_size=128, verbose=2)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple CNN for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 346, 1, 100)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 350, 300)          600900    \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 350, 300, 1)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 346, 1, 100)       150100    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 1, 1, 100)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 250)               25250     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 251       \n",
      "=================================================================\n",
      "Total params: 776,501\n",
      "Trainable params: 776,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# create the model - CNN\n",
    "from keras.layers import Conv2D, Reshape, MaxPooling2D\n",
    "\n",
    "vocab_size = W.shape[0]\n",
    "embedding_size = W.shape[1]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_size, input_length=input_length, weights=[W]))\n",
    "model.add(Reshape((input_length, embedding_size, 1)))\n",
    "\n",
    "# CNN hyperparameters\n",
    "num_filters = 100\n",
    "n_gram = 5\n",
    "filter_size = (n_gram, embedding_size)\n",
    "          \n",
    "model.add(Conv2D(num_filters, filter_size, activation='relu'))\n",
    "print(model.output_shape)\n",
    "model.add(MaxPooling2D(pool_size=(model.output_shape[1], 1)))\n",
    "          \n",
    "model.add(Flatten())\n",
    "model.add(Dense(250, activation='relu'))\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "# plot_model(model, to_file='Sent_simple_CNN.png', show_shapes=False, show_layer_names=True, rankdir='TB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5000 samples, validate on 500 samples\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-fb3199e772cd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Fit the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m# Final evaluation of the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Accuracy: %.2f%%\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=5, batch_size=128, verbose=2)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Non-sequential CNN for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Concatenate\n",
    "\n",
    "# Input layer\n",
    "input_x = Input(shape=(input_length,), dtype=np.int32)\n",
    "\n",
    "# Embedding layer\n",
    "embeddings = Embedding(vocab_size, embedding_size, input_length=input_length,\n",
    "                           weights=[W])(input_x)\n",
    "\n",
    "# Reshape to 3D\n",
    "conv_input = Reshape((input_length, embedding_size, 1))(embeddings)\n",
    "\n",
    "# CNN Hyperparameters\n",
    "# 3 filters shapes\n",
    "filter_widths = [3, 4, 5]\n",
    "\n",
    "num_filters = 100\n",
    "filter_height = embedding_size\n",
    "\n",
    "pooled_outputs = []\n",
    "\n",
    "# Convolution layers\n",
    "for width in filter_widths:\n",
    "    conv = Conv2D(num_filters, (width, filter_height), activation='relu')(conv_input)\n",
    "    pooling = MaxPooling2D(pool_size=(int(conv.shape[1]), 1))(conv)\n",
    "    pooled_outputs.append(pooling)\n",
    "    \n",
    "# Concatenation of maxpooling outputs\n",
    "h_pool = Concatenate(axis=-1)(pooled_outputs)\n",
    "h_pool = Reshape((int(h_pool.shape[-1]), ))(h_pool)\n",
    "\n",
    "# Sigmoid\n",
    "dense = Dense(1, activation='sigmoid')(h_pool)\n",
    "\n",
    "model = Model(input_x, dense)\n",
    "\n",
    "\n",
    "# Model compilation, Adam optimizer\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.summary()    \n",
    "# plot_model(model, to_file='Sent_non_seq_CNN.png', show_shapes=False, show_layer_names=True, rankdir='TB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=5, batch_size=512, verbose=2)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(x_test, y_test,  batch_size=512, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  LSTM-RNN for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "embedding_3 (Embedding)      (None, 10, 32)            64064     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 117,365\n",
      "Trainable params: 117,365\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import LSTM, Input\n",
    "\n",
    "# Input layer\n",
    "input_x = Input(shape=(input_length,), dtype=np.int32)\n",
    "\n",
    "# Embedding layer\n",
    "embeddings = Embedding(2002, 32, input_length=input_length, \n",
    "                       mask_zero=True)(input_x)\n",
    "\n",
    "# Reshape to 3D\n",
    "lstm = LSTM(units=100)(embeddings)\n",
    "\n",
    "\n",
    "# Sigmoid\n",
    "dense = Dense(1, activation='sigmoid')(lstm)\n",
    "\n",
    "# Defining model\n",
    "model = Model(input_x, dense)\n",
    "\n",
    "\n",
    "# Model compilation, Adam optimizer\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.summary()    \n",
    "# plot_model(model, to_file='Sent_lstm.png', show_shapes=False, show_layer_names=True, rankdir='TB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(10)])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "history = model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=5, batch_size=512)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(x_test, y_test, batch_size=512, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting some items in gray scale \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving and loading the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model.save('my_model.h5')  # creates a HDF5 file 'my_model.h5'\n",
    "del model  # deletes the existing model\n",
    "\n",
    "# returns a compiled model\n",
    "# identical to the previous one\n",
    "model = load_model('my_model.h5')\n",
    "\n",
    "scores = model.evaluate(x_test, y_test, batch_size=512, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
