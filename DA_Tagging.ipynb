{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import numpy as np\n",
    "#from keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_data(fname):\n",
    "    with open(fname,'r') as f:\n",
    "        utt_ID = [line.split(None, 1)[0] for line in f]\n",
    "    with open(fname,'r') as f:\n",
    "        y = [line.split('\\t', 2)[1] for line in f]\n",
    "    with open(fname,'r') as f:\n",
    "        x_train_org =[line.strip().split('\\t', 2)[2] for line in f]\n",
    "        x_train = [x.split(';') for x in x_train_org]\n",
    "    return utt_ID,y,x_train \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "196502"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utt_ID_train,y_train,x_train_org= load_data(\"utterances1.train\")\n",
    "x_train_org[0]\n",
    "len(x_train_org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=x_train_org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float32')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v['in'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "dsdvsdv\n",
      "[]\n",
      "dsdvsdv\n",
      "[]\n",
      "dsdvsdv\n",
      "['okay', ',', 'uh']\n",
      "[]\n",
      "dsdvsdv\n",
      "[]\n",
      "dsdvsdv\n",
      "['okay', ',', 'uh']\n",
      "['could', 'you', 'tell', 'me', 'what', 'you', 'think', 'contributes', 'most', 'to', ',', 'uh', ',', 'air', 'pollution', '?']\n",
      "[]\n",
      "dsdvsdv\n",
      "['okay', ',', 'uh']\n",
      "['could', 'you', 'tell', 'me', 'what', 'you', 'think', 'contributes', 'most', 'to', ',', 'uh', ',', 'air', 'pollution', '?']\n",
      "['well', ',', 'it', \"'s\", 'hard', 'to', 'say', '.']\n"
     ]
    }
   ],
   "source": [
    "x_train=np.empty(len(x_train_org),dtype=object)\n",
    "#list()\n",
    "x_tr=[]\n",
    "#x_1=[]\n",
    "for i in range(len(x_train_org)):\n",
    "    x_tr=[]\n",
    "    x_1=[None]*4\n",
    "    x_2=[]\n",
    "    res = []\n",
    "    for list in x_train_org[i]:\n",
    "        res.append(str(list).lower().split())\n",
    "    for j in range(0,4):\n",
    "        print(res[j])\n",
    "        if(len(res[j])==0):\n",
    "            print(\"dsdvsdv\")\n",
    "            x_tr.append(np.zeros(300, dtype=float))\n",
    "            #x_1[j]=np.array(x_tr)\n",
    "            #x_tr.append(np.array(res[j]))\n",
    "        for k in range(0,len(res[j])):\n",
    "            x_2.append(w2v[res[j][k]])\n",
    "            #x_1[j]=np.array(x_tr)\n",
    "    x_train[i]=np.array(x_1)\n",
    "    if (i==2): break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(196502,)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train=list()\n",
    "for i in range(len(x_train_org)):\n",
    "    res = []\n",
    "    for list in x_train_org[i]:\n",
    "        res.append(str(list).lower().split())\n",
    "    x_train[i]=res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21319\n"
     ]
    }
   ],
   "source": [
    "x_train[3][2][2]\n",
    "\n",
    "uniqueWords = []\n",
    "for i in range(len(x_train_org)):\n",
    "        for j in range(len(x_train[i])):\n",
    "             for k in x_train[i][j]:\n",
    "                    if not k.lower() in uniqueWords:\n",
    "                        uniqueWords.append(k.lower())\n",
    "print(len(uniqueWords))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=1\n",
    "vocab ={}\n",
    "for x in uniqueWords:\n",
    "    vocab.update({x:i})\n",
    "    i=i+1\n",
    "#for x in uniqueWords:\n",
    "#    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21319"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in uniqueWords:\n",
    "#    print(i.lower())\n",
    "#INDEX_FROM = 2 \n",
    "\n",
    "# Dict {word:id}\n",
    "word_to_id = vocab#{x:vocab[x]+INDEX_FROM for x in vocab if vocab[x]<=2000}\n",
    "#word_to_id[\"<START>\"] = 1\n",
    "#word_to_id[\"<UNK>\"] = 2\n",
    "\n",
    "\n",
    "# Dict {id:word}\n",
    "id_to_word = {word_to_id[x]:x for x in word_to_id}\n",
    "\n",
    "# Array of ordered words by their frequency + special characters\n",
    "#vocab_list = np.array([\"<PAD>\"]+[id_to_word[x] for x in range(1,2001)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'okay',\n",
       " 2: ',',\n",
       " 3: 'uh',\n",
       " 4: 'could',\n",
       " 5: 'you',\n",
       " 6: 'tell',\n",
       " 7: 'me',\n",
       " 8: 'what',\n",
       " 9: 'think',\n",
       " 10: 'contributes',\n",
       " 11: 'most',\n",
       " 12: 'to',\n",
       " 13: 'air',\n",
       " 14: 'pollution',\n",
       " 15: '?',\n",
       " 16: 'well',\n",
       " 17: 'it',\n",
       " 18: \"'s\",\n",
       " 19: 'hard',\n",
       " 20: 'say',\n",
       " 21: '.',\n",
       " 22: 'i',\n",
       " 23: 'mean',\n",
       " 24: 'while',\n",
       " 25: 'certainly',\n",
       " 26: 'the',\n",
       " 27: 'case',\n",
       " 28: 'that',\n",
       " 29: 'things',\n",
       " 30: 'like',\n",
       " 31: 'automobiles',\n",
       " 32: 'and',\n",
       " 33: 'factories',\n",
       " 34: 'pollute',\n",
       " 35: 'a',\n",
       " 36: 'lot',\n",
       " 37: 'if',\n",
       " 38: 'look',\n",
       " 39: 'at',\n",
       " 40: 'how',\n",
       " 41: 'much',\n",
       " 42: 'is',\n",
       " 43: 'kicked',\n",
       " 44: 'up',\n",
       " 45: 'by',\n",
       " 46: 'an',\n",
       " 47: 'active',\n",
       " 48: 'volcano',\n",
       " 49: 'less',\n",
       " 50: 'than',\n",
       " 51: 'clear',\n",
       " 52: 'anything',\n",
       " 53: 'man',\n",
       " 54: 'can',\n",
       " 55: 'do',\n",
       " 56: 'in',\n",
       " 57: 'this',\n",
       " 58: 'sort',\n",
       " 59: 'of',\n",
       " 60: 'scale',\n",
       " 61: 'has',\n",
       " 62: 'effect',\n",
       " 63: 'all',\n",
       " 64: 'um',\n",
       " 65: 'talked',\n",
       " 66: 'about',\n",
       " 67: 'volcanos',\n",
       " 68: \"'m\",\n",
       " 69: 'not',\n",
       " 70: 'sure',\n",
       " 71: 'many',\n",
       " 72: 'there',\n",
       " 73: 'are',\n",
       " 74: 'now',\n",
       " 75: 'amount',\n",
       " 76: 'material',\n",
       " 77: 'they',\n",
       " 78: 'put',\n",
       " 79: 'into',\n",
       " 80: 'atmosphere',\n",
       " 81: 'probably',\n",
       " 82: 'greatest',\n",
       " 83: 'cause',\n",
       " 84: 'vehicles',\n",
       " 85: 'uh-huh',\n",
       " 86: 'especially',\n",
       " 87: 'around',\n",
       " 88: 'cities',\n",
       " 89: 'live',\n",
       " 90: 'right',\n",
       " 91: 'city',\n",
       " 92: 'itself',\n",
       " 93: 'no',\n",
       " 94: 'more',\n",
       " 95: 'out',\n",
       " 96: 'suburbs',\n",
       " 97: 'but',\n",
       " 98: 'work',\n",
       " 99: 'near',\n",
       " 100: 'so',\n",
       " 101: 'ca-',\n",
       " 102: 'notice',\n",
       " 103: 'rural',\n",
       " 104: 'area',\n",
       " 105: 'mainly',\n",
       " 106: 'farms',\n",
       " 107: 'heavy',\n",
       " 108: 'industry',\n",
       " 109: 'attleboro',\n",
       " 110: 'rhode',\n",
       " 111: 'island',\n",
       " 112: 'oh',\n",
       " 113: 'see',\n",
       " 114: 'north',\n",
       " 115: 'northeast',\n",
       " 116: 'corner',\n",
       " 117: 'sits',\n",
       " 118: 'just',\n",
       " 119: 'over',\n",
       " 120: 'line',\n",
       " 121: 'where',\n",
       " 122: 't',\n",
       " 123: 'plant',\n",
       " 124: \"n't\",\n",
       " 125: 'freeways',\n",
       " 126: 'we',\n",
       " 127: 'get',\n",
       " 128: 'occasional',\n",
       " 129: 'depends',\n",
       " 130: 'which',\n",
       " 131: 'way',\n",
       " 132: 'winds',\n",
       " 133: 'blowing',\n",
       " 134: 'from',\n",
       " 135: 'boston',\n",
       " 136: \"'re\",\n",
       " 137: 'forty',\n",
       " 138: 'miles',\n",
       " 139: 'south',\n",
       " 140: \"'ll\",\n",
       " 141: 'pick',\n",
       " 142: \"'ve\",\n",
       " 143: 'noticed',\n",
       " 144: 'past',\n",
       " 145: 'maybe',\n",
       " 146: 'five',\n",
       " 147: 'or',\n",
       " 148: 'six',\n",
       " 149: 'years',\n",
       " 150: 'twenty',\n",
       " 151: 'away',\n",
       " 152: 'state',\n",
       " 153: 'airport',\n",
       " 154: 'fly',\n",
       " 155: 'patterns',\n",
       " 156: 'jets',\n",
       " 157: 'getting',\n",
       " 158: 'bigger',\n",
       " 159: 'swinging',\n",
       " 160: 'wider',\n",
       " 161: 'coming',\n",
       " 162: 'our',\n",
       " 163: 'homes',\n",
       " 164: 'seems',\n",
       " 165: 'catching',\n",
       " 166: 'residue',\n",
       " 167: 'kerosene',\n",
       " 168: 'dropping',\n",
       " 169: 'other',\n",
       " 170: 'know',\n",
       " 171: 'have',\n",
       " 172: 'unless',\n",
       " 173: 'midwest',\n",
       " 174: 'emissions',\n",
       " 175: 'yeah',\n",
       " 176: 'power',\n",
       " 177: 'plants',\n",
       " 178: 'coal',\n",
       " 179: 'generate',\n",
       " 180: 'one',\n",
       " 181: 'biggest',\n",
       " 182: 'electrical',\n",
       " 183: 'uses',\n",
       " 184: 'electricity',\n",
       " 185: 'does',\n",
       " 186: 'seem',\n",
       " 187: 'be',\n",
       " 188: 'emission',\n",
       " 189: 'them',\n",
       " 190: 'rest',\n",
       " 191: 'country',\n",
       " 192: 'locally',\n",
       " 193: 'major',\n",
       " 194: 'problem',\n",
       " 195: 'kodak',\n",
       " 196: 'interesting',\n",
       " 197: 'because',\n",
       " 198: 'order',\n",
       " 199: 'keep',\n",
       " 200: 'with',\n",
       " 201: 'e',\n",
       " 202: 'p',\n",
       " 203: 'standards',\n",
       " 204: 'tend',\n",
       " 205: 'visible',\n",
       " 206: 'your',\n",
       " 207: 'smokestack',\n",
       " 208: 'their',\n",
       " 209: 'night',\n",
       " 210: 'people',\n",
       " 211: '<laughter>',\n",
       " 212: 'morning',\n",
       " 213: 'neighborhood',\n",
       " 214: 'got',\n",
       " 215: 'black',\n",
       " 216: 'ash',\n",
       " 217: 'on',\n",
       " 218: 'cars',\n",
       " 219: 'surprise',\n",
       " 220: 'really',\n",
       " 221: 'had',\n",
       " 222: 'ball',\n",
       " 223: \"'d\",\n",
       " 224: 'go',\n",
       " 225: 'few',\n",
       " 226: 'phosphorous',\n",
       " 227: 'grenades',\n",
       " 228: 'light',\n",
       " 229: 'sky',\n",
       " 230: 'photograph',\n",
       " 231: 'point',\n",
       " 232: 'must',\n",
       " 233: 'some',\n",
       " 234: 'larger',\n",
       " 235: 'hundred',\n",
       " 236: 'twenty-eight',\n",
       " 237: 'reports',\n",
       " 238: 'during',\n",
       " 239: 'blow',\n",
       " 240: 'off',\n",
       " 241: 'stacks',\n",
       " 242: 'warehouses',\n",
       " 243: 'for',\n",
       " 244: 'powerhouses',\n",
       " 245: 'too',\n",
       " 246: 'employees',\n",
       " 247: 'been',\n",
       " 248: 'complaining',\n",
       " 249: 'pitted',\n",
       " 250: 'spots',\n",
       " 251: 'something',\n",
       " 252: 'a-',\n",
       " 253: 'guess',\n",
       " 254: 'little',\n",
       " 255: 'ridiculous',\n",
       " 256: 'lakes',\n",
       " 257: 'cleaner',\n",
       " 258: 'water',\n",
       " 259: 'directly',\n",
       " 260: 'related',\n",
       " 261: 'acid',\n",
       " 262: 'rain',\n",
       " 263: 'was',\n",
       " 264: 'stuff',\n",
       " 265: 'read',\n",
       " 266: 'recently',\n",
       " 267: 'technology',\n",
       " 268: 'review',\n",
       " 269: 'basically',\n",
       " 270: 'indicates',\n",
       " 271: 'may',\n",
       " 272: 'bit',\n",
       " 273: 'overstated',\n",
       " 274: 'die',\n",
       " 275: 'seen',\n",
       " 276: 'forests',\n",
       " 277: 'due',\n",
       " 278: 'ye-',\n",
       " 279: 'expert',\n",
       " 280: 'did',\n",
       " 281: 'article',\n",
       " 282: 'were',\n",
       " 283: 'dumping',\n",
       " 284: 'lime',\n",
       " 285: 'upstate',\n",
       " 286: 'new',\n",
       " 287: 'york',\n",
       " 288: 'somewhere',\n",
       " 289: 'huge',\n",
       " 290: 'areas',\n",
       " 291: 'thought',\n",
       " 292: 'beneficial',\n",
       " 293: 'soak',\n",
       " 294: 'runs',\n",
       " 295: 'streams',\n",
       " 296: 'rivers',\n",
       " 297: 'fish',\n",
       " 298: 'supposedly',\n",
       " 299: 'making',\n",
       " 300: 'comeback',\n",
       " 301: 'ca',\n",
       " 302: 'remember',\n",
       " 303: 'interesti-',\n",
       " 304: 'hampshire',\n",
       " 305: 'parts',\n",
       " 306: 'vermont',\n",
       " 307: 'showed',\n",
       " 308: 'pictures',\n",
       " 309: 'extensive',\n",
       " 310: 'tree',\n",
       " 311: 'damage',\n",
       " 312: 'attributed',\n",
       " 313: 'kind',\n",
       " 314: 'else',\n",
       " 315: 'then',\n",
       " 316: 'environmentalists',\n",
       " 317: 'claiming',\n",
       " 318: 'though',\n",
       " 319: 'said',\n",
       " 320: 'rains',\n",
       " 321: 'contribution',\n",
       " 322: 'previously',\n",
       " 323: 'suspected',\n",
       " 324: 'natural',\n",
       " 325: 'disease',\n",
       " 326: 'as',\n",
       " 327: 'individuals',\n",
       " 328: 'group',\n",
       " 329: '<<pause>>',\n",
       " 330: 'demand',\n",
       " 331: 'efficient',\n",
       " 332: 'thing',\n",
       " 333: 'still',\n",
       " 334: 'causes',\n",
       " 335: 'pollutants',\n",
       " 336: 'difficult',\n",
       " 337: 'ask',\n",
       " 338: 'automakers',\n",
       " 339: 'pollutant',\n",
       " 340: 'ne-',\n",
       " 341: 'yo-',\n",
       " 342: 'need',\n",
       " 343: 'vehicle',\n",
       " 344: 'ralph',\n",
       " 345: 'nader',\n",
       " 346: 'raiders',\n",
       " 347: 'r',\n",
       " 348: 'members',\n",
       " 349: 'those',\n",
       " 350: 'types',\n",
       " 351: 'groups',\n",
       " 352: 'big',\n",
       " 353: 'reading',\n",
       " 354: 'older',\n",
       " 355: 'polluters',\n",
       " 356: 'twelve',\n",
       " 357: 'fifteen',\n",
       " 358: 'year',\n",
       " 359: 'old',\n",
       " 360: 'contribute',\n",
       " 361: 'ninety',\n",
       " 362: 'percent',\n",
       " 363: 'automobile',\n",
       " 364: 'hardly',\n",
       " 365: 'better',\n",
       " 366: 'trucks',\n",
       " 367: 'buses',\n",
       " 368: 'when',\n",
       " 369: 'last',\n",
       " 370: 'time',\n",
       " 371: 'saw',\n",
       " 372: 'truck',\n",
       " 373: 'belch',\n",
       " 374: 'smoke',\n",
       " 375: 'tune',\n",
       " 376: 'trucking',\n",
       " 377: 'indus-',\n",
       " 378: 'incompetent',\n",
       " 379: 'would',\n",
       " 380: 'fuel',\n",
       " 381: 'costs',\n",
       " 382: 'diesel',\n",
       " 383: 'engines',\n",
       " 384: 'soot',\n",
       " 385: 'least',\n",
       " 386: 'particulate',\n",
       " 387: 'comes',\n",
       " 388: 'pretty',\n",
       " 389: 'quickly',\n",
       " 390: 'also',\n",
       " 391: 'push',\n",
       " 392: 'legislation',\n",
       " 393: 'rapid',\n",
       " 394: 'transit',\n",
       " 395: 'systems',\n",
       " 396: 'behind',\n",
       " 397: 'hand',\n",
       " 398: 'use',\n",
       " 399: 'inconvenient',\n",
       " 400: 'only',\n",
       " 401: 'used',\n",
       " 402: 'single',\n",
       " 403: 'person',\n",
       " 404: 'car',\n",
       " 405: 'driving',\n",
       " 406: 'even',\n",
       " 407: 'carpool',\n",
       " 408: 'help',\n",
       " 409: 'united',\n",
       " 410: 'states',\n",
       " 411: 'canada',\n",
       " 412: 'going',\n",
       " 413: 'agreements',\n",
       " 414: 'limit',\n",
       " 415: 'being',\n",
       " 416: 'given',\n",
       " 417: 'quite',\n",
       " 418: 'concerned',\n",
       " 419: 'sending',\n",
       " 420: 'helping',\n",
       " 421: 'device',\n",
       " 422: 'meter',\n",
       " 423: 'tail',\n",
       " 424: 'pipe',\n",
       " 425: 'pay',\n",
       " 426: 'tax',\n",
       " 427: 'based',\n",
       " 428: 'polluted',\n",
       " 429: 'idea',\n",
       " 430: 'loud',\n",
       " 431: 'every',\n",
       " 432: 'town',\n",
       " 433: 'will',\n",
       " 434: 'generating',\n",
       " 435: 'revenue',\n",
       " 436: 'means',\n",
       " 437: 'very',\n",
       " 438: 'any',\n",
       " 439: 'buy',\n",
       " 440: 'these',\n",
       " 441: 'devise',\n",
       " 442: 'slowly',\n",
       " 443: 'closed',\n",
       " 444: 'longer',\n",
       " 445: 'lasts',\n",
       " 446: 'start',\n",
       " 447: 'anymore',\n",
       " 448: 'concept',\n",
       " 449: 'difficulty',\n",
       " 450: 'somebody',\n",
       " 451: 'obviously',\n",
       " 452: 'take',\n",
       " 453: 'again',\n",
       " 454: 'build',\n",
       " 455: 'muffler',\n",
       " 456: 'installed',\n",
       " 457: 'catalytic',\n",
       " 458: 'converter',\n",
       " 459: 'mufflers',\n",
       " 460: 'obvious',\n",
       " 461: 'took',\n",
       " 462: 'replace',\n",
       " 463: 'built',\n",
       " 464: 'talking',\n",
       " 465: 'universal',\n",
       " 466: 'busses',\n",
       " 467: '<throat',\n",
       " 468: 'clearing>',\n",
       " 469: 'penalize',\n",
       " 470: 'should',\n",
       " 471: 'pursue',\n",
       " 472: 'patent',\n",
       " 473: 'good',\n",
       " 474: 'come',\n",
       " 475: 'solve',\n",
       " 476: 'problems',\n",
       " 477: 'approached',\n",
       " 478: 'keen',\n",
       " 479: 'installing',\n",
       " 480: 'proposal',\n",
       " 481: 'proper',\n",
       " 482: 'authorities',\n",
       " 483: 'might',\n",
       " 484: 'regulatory',\n",
       " 485: 'agencies',\n",
       " 486: 'interest',\n",
       " 487: 'involved',\n",
       " 488: 'through',\n",
       " 489: 'organization',\n",
       " 490: 'heard',\n",
       " 491: 'myself',\n",
       " 492: 'either',\n",
       " 493: 'its',\n",
       " 494: 'seriously',\n",
       " 495: 'always',\n",
       " 496: 'felt',\n",
       " 497: 'long',\n",
       " 498: 'young',\n",
       " 499: 'kids',\n",
       " 500: 'ought',\n",
       " 501: 'stint',\n",
       " 502: 'primarily',\n",
       " 503: 'military',\n",
       " 504: 'countries',\n",
       " 505: 'require',\n",
       " 506: 'mandatory',\n",
       " 507: 'men',\n",
       " 508: 'women',\n",
       " 509: 'serve',\n",
       " 510: 'israel',\n",
       " 511: 'everybody',\n",
       " 512: 'bad',\n",
       " 513: 'teaches',\n",
       " 514: 'went',\n",
       " 515: 'service',\n",
       " 516: 'eighteen',\n",
       " 517: 'stayed',\n",
       " 518: 'ten',\n",
       " 519: 'growing',\n",
       " 520: 'bet',\n",
       " 521: 'my',\n",
       " 522: 'father',\n",
       " 523: 'he',\n",
       " 524: 'himself',\n",
       " 525: 'school',\n",
       " 526: 'came',\n",
       " 527: 'poor',\n",
       " 528: 'immigrant',\n",
       " 529: 'family',\n",
       " 530: 'chance',\n",
       " 531: 'wealthy',\n",
       " 532: 'successful',\n",
       " 533: 'parents',\n",
       " 534: 'wanted',\n",
       " 535: 'send',\n",
       " 536: 'college',\n",
       " 537: 'dead',\n",
       " 538: 'set',\n",
       " 539: 'against',\n",
       " 540: '<breathing>',\n",
       " 541: 'wanderlust',\n",
       " 542: 'after',\n",
       " 543: 'wasting',\n",
       " 544: 'first',\n",
       " 545: 'partying',\n",
       " 546: 'everything',\n",
       " 547: 'decided',\n",
       " 548: 'settle',\n",
       " 549: 'down',\n",
       " 550: 'started',\n",
       " 551: 'education',\n",
       " 552: 'course',\n",
       " 553: 'job',\n",
       " 554: 'such',\n",
       " 555: 'ended',\n",
       " 556: 'two',\n",
       " 557: 'semesters',\n",
       " 558: 'degree',\n",
       " 559: 'ever',\n",
       " 560: 'since',\n",
       " 561: 'kinds',\n",
       " 562: 'done',\n",
       " 563: 'working',\n",
       " 564: 'national',\n",
       " 565: 'parks',\n",
       " 566: 'clean',\n",
       " 567: 'roadsides',\n",
       " 568: 'exactly',\n",
       " 569: 'welfare',\n",
       " 570: 'required',\n",
       " 571: 'spend',\n",
       " 572: 'part',\n",
       " 573: 'why',\n",
       " 574: 'slash',\n",
       " 575: 'absol-',\n",
       " 576: 'takes',\n",
       " 577: 'opportunity',\n",
       " 578: '<beep>',\n",
       " 579: '<<three',\n",
       " 580: 'times>>',\n",
       " 581: 'roads',\n",
       " 582: 'absolutely',\n",
       " 583: 'money',\n",
       " 584: 'throwing',\n",
       " 585: 'paying',\n",
       " 586: 'recipients',\n",
       " 587: 'able-bodied',\n",
       " 588: 'having',\n",
       " 589: 'nothing',\n",
       " 590: 'together',\n",
       " 591: 'folks',\n",
       " 592: 'places',\n",
       " 593: 'let',\n",
       " 594: 'cut',\n",
       " 595: 'yards',\n",
       " 596: 'paint',\n",
       " 597: 'houses',\n",
       " 598: 'learn',\n",
       " 599: 'benefit',\n",
       " 600: 'next',\n",
       " 601: 'door',\n",
       " 602: 'neighbor',\n",
       " 603: 'worked',\n",
       " 604: 'unemployment',\n",
       " 605: 'division',\n",
       " 606: 'his',\n",
       " 607: 'sole',\n",
       " 608: 'tracking',\n",
       " 609: 'who',\n",
       " 610: 'benefits',\n",
       " 611: 'horror',\n",
       " 612: 'stories',\n",
       " 613: 'guys',\n",
       " 614: 'checks',\n",
       " 615: 'back',\n",
       " 616: 'days',\n",
       " 617: 'give',\n",
       " 618: 'booklets',\n",
       " 619: 'clothing',\n",
       " 620: 'gas',\n",
       " 621: 'sit',\n",
       " 622: 'poker',\n",
       " 623: 'games',\n",
       " 624: 'jus-',\n",
       " 625: 'dealing',\n",
       " 626: 'hear',\n",
       " 627: 'paul',\n",
       " 628: 'harvey',\n",
       " 629: 'radio',\n",
       " 630: 'segment',\n",
       " 631: 'goes',\n",
       " 632: 'each',\n",
       " 633: 'without',\n",
       " 634: 'middle',\n",
       " 635: 'government',\n",
       " 636: 'bureaucracy',\n",
       " 637: 'receive',\n",
       " 638: 'forty-five',\n",
       " 639: 'thousand',\n",
       " 640: 'dollars',\n",
       " 641: 'astounded',\n",
       " 642: 'make',\n",
       " 643: 'layer',\n",
       " 644: 'between',\n",
       " 645: 'recipient',\n",
       " 646: 'taking',\n",
       " 647: 'easily',\n",
       " 648: 'seven',\n",
       " 649: 'times',\n",
       " 650: 'spent',\n",
       " 651: 'doubt',\n",
       " 652: 'totally',\n",
       " 653: 'graduate',\n",
       " 654: 'public',\n",
       " 655: 'administration',\n",
       " 656: 'honestly',\n",
       " 657: 'fed',\n",
       " 658: 'stand',\n",
       " 659: 'writing',\n",
       " 660: 'budgets',\n",
       " 661: 'wonder',\n",
       " 662: 'x',\n",
       " 663: 'number',\n",
       " 664: 'administer',\n",
       " 665: 'program',\n",
       " 666: 'sad',\n",
       " 667: 'pathetic',\n",
       " 668: 'different',\n",
       " 669: 'never',\n",
       " 670: 'happen',\n",
       " 671: 'here',\n",
       " 672: 'voting',\n",
       " 673: 'blocks',\n",
       " 674: 'return',\n",
       " 675: 'assistance',\n",
       " 676: 'far',\n",
       " 677: 'strictly',\n",
       " 678: 'tuition',\n",
       " 679: 'incentive',\n",
       " 680: 'educational',\n",
       " 681: 'vocational',\n",
       " 682: 'training',\n",
       " 683: 'along',\n",
       " 684: 'eventually',\n",
       " 685: 'rather',\n",
       " 686: 'bunch',\n",
       " 687: 'uneducated',\n",
       " 688: 'educated',\n",
       " 689: 'statistics',\n",
       " 690: 'americans',\n",
       " 691: 'adults',\n",
       " 692: 'lowest',\n",
       " 693: 'rates',\n",
       " 694: 'civilized',\n",
       " 695: 'science',\n",
       " 696: 'math',\n",
       " 697: 'becoming',\n",
       " 698: 'opposed',\n",
       " 699: 'ago',\n",
       " 700: 'industrial',\n",
       " 701: 'giant',\n",
       " 702: 'no-',\n",
       " 703: 'beating',\n",
       " 704: 'us',\n",
       " 705: 'game',\n",
       " 706: 'taught',\n",
       " 707: 'simple',\n",
       " 708: 'ambition',\n",
       " 709: 'lose',\n",
       " 710: 'terrible',\n",
       " 711: 'serving',\n",
       " 712: 'greedy',\n",
       " 713: 'god',\n",
       " 714: 'mall',\n",
       " 715: 'fourteen',\n",
       " 716: 'flaunting',\n",
       " 717: 'play',\n",
       " 718: 'video',\n",
       " 719: 'teach',\n",
       " 720: 'house',\n",
       " 721: 'complaint',\n",
       " 722: 'relinquished',\n",
       " 723: 'parenting',\n",
       " 724: 'subjects',\n",
       " 725: 'trouble',\n",
       " 726: 'substitute',\n",
       " 727: 'teacher',\n",
       " 728: 'yes',\n",
       " 729: 'another',\n",
       " 730: 'master',\n",
       " 731: 'teaching',\n",
       " 732: 'love',\n",
       " 733: 'fun',\n",
       " 734: 'helped',\n",
       " 735: 'learned',\n",
       " 736: 'differently',\n",
       " 737: 'boy',\n",
       " 738: 'want',\n",
       " 739: 'bedroom',\n",
       " 740: 'apartment',\n",
       " 741: 'life',\n",
       " 742: 'selfish',\n",
       " 743: 'admit',\n",
       " 744: 'yourself',\n",
       " 745: 'some-',\n",
       " 746: 'garbage',\n",
       " 747: 'teachers',\n",
       " 748: 'wrong',\n",
       " 749: 'gosh',\n",
       " 750: 'saying',\n",
       " 751: 'paid',\n",
       " 752: 'necessary',\n",
       " 753: 'great',\n",
       " 754: 'deal',\n",
       " 755: 'admiration',\n",
       " 756: 'sense',\n",
       " 757: 'priorities',\n",
       " 758: 'important',\n",
       " 759: 'picking',\n",
       " 760: 'educating',\n",
       " 761: 'both',\n",
       " 762: 'place',\n",
       " 763: 'gets',\n",
       " 764: 'import-',\n",
       " 765: 'starts',\n",
       " 766: 'twenty-six',\n",
       " 767: 'whole',\n",
       " 768: 'homeless',\n",
       " 769: 'thirteen',\n",
       " 770: 'pet',\n",
       " 771: 'peeve',\n",
       " 772: 'unions',\n",
       " 773: 'gone',\n",
       " 774: 'needed',\n",
       " 775: 'served',\n",
       " 776: 'function',\n",
       " 777: 'enough',\n",
       " 778: 'laws',\n",
       " 779: 'books',\n",
       " 780: 'outdated',\n",
       " 781: 'perpetuate',\n",
       " 782: 'own',\n",
       " 783: 'structure',\n",
       " 784: 'company',\n",
       " 785: 'dad',\n",
       " 786: 'strike',\n",
       " 787: 'management',\n",
       " 788: 'steel',\n",
       " 789: 'manufacturing',\n",
       " 790: 'import',\n",
       " 791: 'cheap',\n",
       " 792: 'told',\n",
       " 793: 'strikers',\n",
       " 794: 'business',\n",
       " 795: 'bankrupt',\n",
       " 796: 'lost',\n",
       " 797: 'jobs',\n",
       " 798: 'happened',\n",
       " 799: 'miners',\n",
       " 800: 'workers',\n",
       " 801: 'same',\n",
       " 802: 'care',\n",
       " 803: 'detroit',\n",
       " 804: 'awfully',\n",
       " 805: 'stopping',\n",
       " 806: 'trying',\n",
       " 807: 'imports',\n",
       " 808: 'caught',\n",
       " 809: 'tough',\n",
       " 810: 'road',\n",
       " 811: 'partnership',\n",
       " 812: 'japanese',\n",
       " 813: 'embarrassed',\n",
       " 814: 'original',\n",
       " 815: 'subject',\n",
       " 816: 'youngsters',\n",
       " 817: 'pride',\n",
       " 818: 'themselves',\n",
       " 819: 'organize',\n",
       " 820: 'within',\n",
       " 821: 'community',\n",
       " 822: 'eagle',\n",
       " 823: 'scouts',\n",
       " 824: 'manpower',\n",
       " 825: 'pool',\n",
       " 826: '<tv>',\n",
       " 827: 'opinion',\n",
       " 828: 'gun',\n",
       " 829: 'control',\n",
       " 830: 'mixed',\n",
       " 831: 'emotions',\n",
       " 832: 'listen',\n",
       " 833: 'watch',\n",
       " 834: 'v',\n",
       " 835: 'happening',\n",
       " 836: 'token',\n",
       " 837: 'purchased',\n",
       " 838: 'rifle',\n",
       " 839: 'ammunition',\n",
       " 840: 'afraid',\n",
       " 841: 'bought',\n",
       " 842: 'target',\n",
       " 843: 'practicing',\n",
       " 844: 'weapon',\n",
       " 845: 'feel',\n",
       " 846: 'texas',\n",
       " 847: 'waiting',\n",
       " 848: 'period',\n",
       " 849: 'handguns',\n",
       " 850: 'huh',\n",
       " 851: 'pistol',\n",
       " 852: 'twenty-two',\n",
       " 853: 'gauge',\n",
       " 854: 'shotgun',\n",
       " 855: 'an-',\n",
       " 856: 'guns',\n",
       " 857: 'bullets',\n",
       " 858: 'three',\n",
       " 859: '<sniffing>',\n",
       " 860: '<child>',\n",
       " 861: 'shot',\n",
       " 862: '<swallowing>',\n",
       " 863: '<child',\n",
       " 864: 'crying>',\n",
       " 865: 'handy',\n",
       " 866: 'england',\n",
       " 867: 'permit',\n",
       " 868: 'before',\n",
       " 869: 'amu-',\n",
       " 870: 'am-',\n",
       " 871: 'talking>',\n",
       " 872: 'weeks',\n",
       " 873: 'system',\n",
       " 874: 'process',\n",
       " 875: 'request',\n",
       " 876: 'police',\n",
       " 877: 'station',\n",
       " 878: 'local',\n",
       " 879: 'nice',\n",
       " 880: '<snorting>',\n",
       " 881: 'criminals',\n",
       " 882: 'agree',\n",
       " 883: 'needs',\n",
       " 884: 'stop',\n",
       " 885: 'impulse',\n",
       " 886: 'buyer',\n",
       " 887: 'buying',\n",
       " 888: '<',\n",
       " 889: 'child',\n",
       " 890: 'putting',\n",
       " 891: 'week',\n",
       " 892: 'someone',\n",
       " 893: 'determined',\n",
       " 894: 'firearm',\n",
       " 895: '<inhaling>',\n",
       " 896: 'tennessee',\n",
       " 897: 'nine',\n",
       " 898: 'nature',\n",
       " 899: 'react',\n",
       " 900: 'words',\n",
       " 901: 'shoot',\n",
       " 902: 'day',\n",
       " 903: 'th-',\n",
       " 904: 'wants',\n",
       " 905: 'firearms',\n",
       " 906: 'difference',\n",
       " 907: 'whether',\n",
       " 908: 'suppose',\n",
       " 909: '<dishes>',\n",
       " 910: 'hurts',\n",
       " 911: 'worse',\n",
       " 912: 'recover',\n",
       " 913: 'spread',\n",
       " 914: 'ours',\n",
       " 915: 'bo-',\n",
       " 916: 'bottom',\n",
       " 917: 'top',\n",
       " 918: 'barrel',\n",
       " 919: 'turn',\n",
       " 920: 'tight',\n",
       " 921: 'loose',\n",
       " 922: 'changes',\n",
       " 923: 'manual',\n",
       " 924: 'automatic',\n",
       " 925: 'features',\n",
       " 926: 'further',\n",
       " 927: 'closer',\n",
       " 928: 'tighter',\n",
       " 929: 'hell',\n",
       " 930: 'kick',\n",
       " 931: 'husband',\n",
       " 932: 'almost',\n",
       " 933: 'knocked',\n",
       " 934: 'li-',\n",
       " 935: 'don-',\n",
       " 936: 'forget',\n",
       " 937: 'fifty-seven',\n",
       " 938: 'belong',\n",
       " 939: 'club',\n",
       " 940: 'belonged',\n",
       " 941: 'awhile',\n",
       " 942: 'moved',\n",
       " 943: 'dishes>',\n",
       " 944: 'find',\n",
       " 945: 'kidding',\n",
       " 946: 'practice',\n",
       " 947: 'traded',\n",
       " 948: 'clubs',\n",
       " 949: 'expensive',\n",
       " 950: 'worth',\n",
       " 951: 'sand',\n",
       " 952: 'bag',\n",
       " 953: 'targets',\n",
       " 954: 'competition',\n",
       " 955: 'joined',\n",
       " 956: 'desire',\n",
       " 957: 'annual',\n",
       " 958: 'membership',\n",
       " 959: 'fee',\n",
       " 960: 'fairly',\n",
       " 961: 'high',\n",
       " 962: 'plus',\n",
       " 963: 'ini-',\n",
       " 964: 'initiation',\n",
       " 965: 'member',\n",
       " 966: 'cost',\n",
       " 967: 'fifty',\n",
       " 968: 'join',\n",
       " 969: 'tournaments',\n",
       " 970: 'doing',\n",
       " 971: '<noise>',\n",
       " 972: '<<ca',\n",
       " 973: 'cough',\n",
       " 974: 'hit>>',\n",
       " 975: 'mostly',\n",
       " 976: 'paper',\n",
       " 977: 'speed',\n",
       " 978: 'accuracy',\n",
       " 979: 'useful',\n",
       " 980: 'purpose',\n",
       " 981: 'fact',\n",
       " 982: 'somehow',\n",
       " 983: 'tie',\n",
       " 984: 'show',\n",
       " 985: 'class',\n",
       " 986: 'handle',\n",
       " 987: 'wait',\n",
       " 988: 'drive',\n",
       " 989: 'him',\n",
       " 990: 'license',\n",
       " 991: 'waited',\n",
       " 992: 'mishandle',\n",
       " 993: 'leave',\n",
       " 994: 'home',\n",
       " 995: 'living',\n",
       " 996: 'navy',\n",
       " 997: 'base',\n",
       " 998: 'memphis',\n",
       " 999: 'guy',\n",
       " 1000: 'lonesome',\n",
       " ...}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_W(word_vecs, k=300):\n",
    "    \"\"\"\n",
    "    Get word matrix. W[i] is the vector for word indexed by i\n",
    "    \"\"\"\n",
    "    vocab_size = len(word_vecs)\n",
    "    word_idx_map = dict()\n",
    "    W = np.zeros(shape=(vocab_size + 1, k), dtype='float32')\n",
    "    W[0] = np.zeros(k, dtype='float32')\n",
    "    i = 1\n",
    "    for word in word_vecs:\n",
    "        W[i] = word_vecs[word]\n",
    "        word_idx_map[word] = i\n",
    "        i += 1\n",
    "    return W, word_idx_map\n",
    "\n",
    "\n",
    "def load_bin_vec(fname, vocab):\n",
    "    \"\"\"\n",
    "    Loads 300x1 word vecs from Google (Mikolov) word2vec\n",
    "    \"\"\"\n",
    "    word_vecs = {}\n",
    "    with open(fname, \"rb\") as f:\n",
    "        header = f.readline()\n",
    "        # ~ print(header)\n",
    "        vocab_size, layer1_size = map(int, header.split())\n",
    "        binary_len = np.dtype('float32').itemsize * layer1_size\n",
    "        # print(vocab_size)\n",
    "        for line in range(vocab_size):\n",
    "            # print(line)\n",
    "            word = []\n",
    "            while True:\n",
    "                ch = f.read(1).decode('iso-8859-1')\n",
    "                if ch == ' ':\n",
    "                    word = ''.join(word)\n",
    "                    break\n",
    "                if ch != '\\n':\n",
    "                    word.append(ch)\n",
    "            # print(word)\n",
    "            if word in vocab:\n",
    "                # print(word)\n",
    "                word_vecs[word] = np.frombuffer(f.read(binary_len), dtype='float32')\n",
    "            else:\n",
    "                f.read(binary_len)\n",
    "\n",
    "    return word_vecs\n",
    "\n",
    "def add_unknown_words(word_vecs, vocab, min_df=1, k=300):\n",
    "    \"\"\"\n",
    "    For words that occur in at least min_df documents, create a separate word vector.    \n",
    "    0.25 is chosen so the unknown vectors have (approximately) same variance as pre-trained ones\n",
    "    \"\"\"\n",
    "    for word in vocab:\n",
    "        if word not in word_vecs and vocab[word] >= min_df:\n",
    "            word_vecs[word] = np.random.uniform(-0.25, 0.25, k)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_list = np.array([id_to_word[x] for x in range(1,21319)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num words found: 17911\n",
      "W shape: (21320, 300)\n",
      "61 seconds to get the embeddings\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "w2v_file = \"GoogleNews-vectors-negative300.bin\"\n",
    "w2v = load_bin_vec(w2v_file, word_to_id)\n",
    "print(\"num words found: %d\" % len(w2v))\n",
    "add_unknown_words(w2v, word_to_id, k=300)\n",
    "W, word_idx_map = get_W(w2v, k=300)\n",
    "\n",
    "print(\"W shape: %s\" % str(W.shape))\n",
    "\n",
    "print(\"%d seconds to get the embeddings\" % (time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['what']\n",
      "['i']\n",
      "['but']\n",
      "['it']\n",
      "['would']\n",
      "['what']\n",
      "['would']\n",
      "['serve']\n",
      "['would']\n",
      "['serve']\n",
      "['both']\n",
      "['purposes']\n",
      "[',']\n",
      "['is']\n",
      "['if']\n",
      "['you']\n",
      "['contact']\n",
      "['him']\n",
      "['and']\n",
      "['ask']\n",
      "['him']\n",
      "['if']\n",
      "['he']\n",
      "[\"'s\"]\n",
      "['already']\n",
      "['done']\n",
      "['it']\n",
      "['.']\n",
      "['yeah']\n",
      "[',']\n",
      "['right']\n",
      "['.']\n",
      "['if']\n",
      "['he']\n",
      "['has']\n",
      "['then']\n",
      "['you']\n",
      "['learn']\n",
      "[',']\n",
      "['oh']\n",
      "['.']\n",
      "['the']\n",
      "['ones']\n",
      "['that']\n",
      "['are']\n",
      "['finished']\n",
      "['being']\n",
      "['transcribed']\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'transcribed'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-748aad997e84>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m                 \u001b[0mx_v\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m                 \u001b[0mx_v\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw2v\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m     \u001b[0mx_valid\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_v\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'transcribed'"
     ]
    }
   ],
   "source": [
    "x_valid=np.empty(len(x_valid_org),dtype=object)\n",
    "#list()\n",
    "x_v=[]\n",
    "for i in range(len(x_valid_org)):\n",
    "    x_v=[]\n",
    "    res = []\n",
    "    for list in x_valid_org[i]:\n",
    "        res.append(str(list).lower().split())\n",
    "        #x_train[i]=res\n",
    "    for j in range(0,4):\n",
    "        #print(res[j])\n",
    "        if(len(res[j])==0):\n",
    "            #print(\"ffffff\")\n",
    "            x_v.append(np.array(res[j]))\n",
    "        for k in range(0,len(res[j])):\n",
    "            #print([res[j][k]])\n",
    "            if(res[j][k] == '[]'):\n",
    "                x_v.append(np.array(res[j]))\n",
    "            else:\n",
    "                x_v.append(w2v[res[j][k]])  \n",
    "    x_valid[i]=np.array(x_v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utt_ID_valid,y_valid,x_valid_org= load_data(\"utterances.valid\")\n",
    "x_valid_org[0]\n",
    "len(x_valid_org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_valid_org' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-5d97b1e2c61a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#x_train=list()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_valid_org\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mlist\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx_valid_org\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mres\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'x_valid_org' is not defined"
     ]
    }
   ],
   "source": [
    "#x_train=list()\n",
    "for i in range(len(x_valid_org)):\n",
    "    res = []\n",
    "    for list in x_valid_org[i]:\n",
    "        res.append(str(list).lower().split())\n",
    "    x_valid[i]=res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(x_valid_org)):\n",
    "        for j in range(len(x_valid[i])):\n",
    "             for k in x_valid[i][j]:\n",
    "                    if not k.lower() in uniqueWords:\n",
    "                        uniqueWords.append(k.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'transcribed'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-5e752905e910>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mw2v\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'transcribed'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m: 'transcribed'"
     ]
    }
   ],
   "source": [
    "w2v['transcribed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max 104\n",
      "mean 9\n"
     ]
    }
   ],
   "source": [
    "lengths = []\n",
    "for i in x_train:\n",
    "    for j in i:\n",
    "        lengths.append(len(j))\n",
    "\n",
    "print(\"max %d\" % max(lengths))\n",
    "print(\"mean %d\" % np.mean(lengths))\n",
    "max_length=max(lengths)\n",
    "mean_length=np.int(np.mean(lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max 11\n",
      "mean 7\n"
     ]
    }
   ],
   "source": [
    "# Max and avg number of word\n",
    "lengths = [len(x) for x in x_train[i] for i in range(len(x_train))]\n",
    "print(\"max %d\" % max(lengths))\n",
    "print(\"mean %d\" % np.mean(lengths))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['%' '%--' '2' 'aa' 'aap' 'ar' 'b' 'ba' 'bc' 'bd' 'bh' 'bk' 'br' 'bs' 'cc'\n",
      " 'co' 'd' 'fa' 'ft' 'g' 'h' 'no' 'qh' 'qo' 'qrr' 'qw' 'qy' 's' 't1' 't3'\n",
      " 'x'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Summarize number of classes\n",
    "print (\"Classes:\", np.unique(y_train), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 10)                3252440   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 31)                341       \n",
      "=================================================================\n",
      "Total params: 3,252,781\n",
      "Trainable params: 3,252,781\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import LSTM, Input\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "#input_length=4\n",
    "model=Sequential()\n",
    "time_steps=4\n",
    "features=300*271\n",
    "\n",
    "model.add(LSTM(units= 10, input_shape=(time_steps,features), activation='tanh'))\n",
    "#model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(31, activation='softmax'))\n",
    "print(model.summary())\n",
    "# plot_model(model, to_file='Sent_FF.png', show_shapes=False, show_layer_names=True, rankdir='TB')\n",
    "\n",
    "# loss function = binary_crossentropy\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6,)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#x_train = np.reshape()\n",
    "x_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected lstm_1_input to have 3 dimensions, but got array with shape (196502, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-08f23819d90e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Fit the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m# Final evaluation of the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#scores = model.evaluate(x_test, y_test, verbose=0)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#print(\"Accuracy: %.2f%%\" % (scores[1]*100))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 952\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m    953\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 751\u001b[1;33m             exception_prefix='input')\n\u001b[0m\u001b[0;32m    752\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    753\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    126\u001b[0m                         \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' dimensions, but got array '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[0;32m    129\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected lstm_1_input to have 3 dimensions, but got array with shape (196502, 1)"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "model.fit(x_train, y_train, validation_data=(x_train, y_train), epochs=2, batch_size=128, verbose=2)\n",
    "# Final evaluation of the model\n",
    "#scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "#print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_valid=x_valid_org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
