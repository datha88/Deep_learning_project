{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![PyTorch Packages](figures/pytorch-logo.png)\n",
    "\n",
    "# PyTorch Tutorial - CNN for Sentiment Analysis\n",
    "\n",
    "Based the PyTorch tutorial from WS17 by Glorianna Jagfeld\n",
    "\n",
    "This notebook provides code to train and evaluate a CNN-based model for sentiment analysis using PyTorch.\n",
    "\n",
    "In order to do so, we will follow these steps:\n",
    "\n",
    "1. Inspect task and data and decide on model\n",
    "\n",
    "2. Implement model/computation graph (forward pass)\n",
    "\n",
    "3. Reading in and preprocessing of data\n",
    "\n",
    "4. Training code\n",
    "\n",
    "5. Evaluation code\n",
    "\n",
    "## 1) Task and Data\n",
    "\n",
    "The task is to assign a binary sentiment label (0 - negative, 1 - positive) to statements taken from film reviews.\n",
    "\n",
    "The data was taken from keras (IMDB dataset) and preprocessed as done in our keras tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Model\n",
    "- We will represent each word in a sentence by a number (index), to be later mapped to a dense vector from an embedding matrix.\n",
    "- The sentence is then represented by a dense matrix, where each row is a word embedding.\n",
    "- We extract features from trigrams of the sentence by using a CNN with filter height 3 and filter width equal to the embedding size\n",
    "- We apply (2,1) maximum pooling to keep the information of the 50% most relevant trigrams from each feature map.\n",
    "- To classify the sentiment, we feed the CNN output through a fully connected layer with 2 output units and apply softmax activation to get the most likely sentiment label.\n",
    "\n",
    "The figure below illustrates the model.\n",
    "Our model differs in the following points from the illustration:\n",
    "- Our sentence matrix will be of size max_sentence_length x 300 (embedding size -word2vec)\n",
    "- We will use only one region size/ filter height (3)\n",
    "- We use 100 filters\n",
    "- We do not apply 1-max pooling but (2,1) max pooling on the CNN output\n",
    "\n",
    "![Model](figures/model.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional, init\n",
    "import random\n",
    "\n",
    "# CNN Model definition (1 convolutional layer)\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, seq_len, num_classes, num_filters, region_size, W):\n",
    "        \"\"\"\n",
    "        :param vocab_size: \n",
    "        :param embedding_size: \n",
    "        :param seq_len: number of tokens (must be constant for all batches)\n",
    "        :param num_classes:\n",
    "        :param num_filters: \n",
    "        :param region_size: \n",
    "        \"\"\"\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        # Embedding Layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        # Load pretrained embeddings, word2vec in this case\n",
    "        self.embedding.weight = nn.Parameter(torch.from_numpy(W))\n",
    "        \n",
    "        # filter width = embedding_size -> extract features from n-grams of size region_size\n",
    "        self.filter_size = (region_size, embedding_size)\n",
    "        # pad height to extract only features from complete n-grams\n",
    "        self.padding = (region_size-1, 0)\n",
    "\n",
    "        # pack together several subsequent operations in one layer\n",
    "        # layer input: [batch, 1, seq_len, embedding_size]\n",
    "        # output: [batch, num_filters, H_{out}, W_{out}]\n",
    "        # H_{out} = seq_len/2+1 because of kernel_size and padding\n",
    "        # W_{out} = 1 because kernel height = embedding_size\n",
    "        self.layer = nn.Sequential(\n",
    "            # in_channels=1, out_channels=num_filters\n",
    "            nn.Conv2d(1, num_filters, kernel_size=self.filter_size, padding=self.padding),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2,1)) # half size of each feature map\n",
    "        )\n",
    "        \n",
    "        self.max_pool_out_size = int(seq_len/2 +1)\n",
    "\n",
    "        # dense layer\n",
    "        self.fc = nn.Linear(self.max_pool_out_size * num_filters, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #[batch, seq_len] -> [batch, seq_len, embedding_size]\n",
    "        embedded = self.embedding(x)\n",
    "        # add channel dimension:\n",
    "        # [batch, seq_len, embedding_size] -> [batch, 1, seq_len, embedding_size]\n",
    "        embedded = embedded.unsqueeze(1)\n",
    "\n",
    "        # [batch, 1, seq_len, embedding_size] -> [batch, num_filters, (seq_len/2)+1, 1]\n",
    "        out = self.layer(embedded)\n",
    "\n",
    "        # [batch,num_filters, (seq_len/2)+1, 1] -> [batch, num_filters * ((seq_len/2) +1)]\n",
    "        out = out.view(out.size(0), -1)\n",
    "\n",
    "        # [batch, num_filters * ((seq_len/2) +1)] -> [batch, num_classes]\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Data reading and preprocessing\n",
    "We have to read in the movie review sentences (IMDB) and their corresponding labels from keras datasets.\n",
    "\n",
    "For the review sentences we have to take care of the following:\n",
    "- The tokens in the sentences are aleready as indices of a fixed vocabulary.\n",
    "- All sentences have to be padded to the same length, such that the CNN layer always yields output vectors of the same size to the dense layer.\n",
    "- Word2Vec embeddings are extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras code\n",
    "_ = ''' from keras.datasets import imdb\n",
    "\n",
    "# Loading the IMBD dataset\n",
    "# Selecting the 2000 most frequent words\n",
    "(x_train_org, y_train), (x_test_org, y_test) = imdb.load_data(path=\"imdb.npz\",\n",
    "                                                      num_words=2000,\n",
    "                                                      skip_top=0,\n",
    "                                                      maxlen=None,\n",
    "                                                      seed=113,\n",
    "                                                      start_char=1,\n",
    "                                                      oov_char=2,\n",
    "                                                      index_from=2)\n",
    "\n",
    "# Loading the vocabulary\n",
    "import numpy as np\n",
    "\n",
    "vocab = imdb.get_word_index(path=\"./imdb_word_index.json\")\n",
    "print \"Number of unique words:\", len(vocab)\n",
    "\n",
    "INDEX_FROM = 2 \n",
    "\n",
    "# Dict {word:id}\n",
    "word_to_id = {x:vocab[x]+INDEX_FROM for x in vocab if vocab[x]<=2000}\n",
    "word_to_id[\"<START>\"] = 1\n",
    "word_to_id[\"<UNK>\"] = 2\n",
    "\n",
    "\n",
    "# Dict {id:word}\n",
    "id_to_word = {word_to_id[x]:x for x in word_to_id}\n",
    "\n",
    "# Array of ordered words by their frequency + special characters\n",
    "vocab_list = np.array([\"<PAD>\"]+[id_to_word[x] for x in range(1,2001)])\n",
    "\n",
    "def get_W(word_vecs, k=300):\n",
    "    \"\"\"\n",
    "    Get word matrix. W[i] is the vector for word indexed by i\n",
    "    \"\"\"\n",
    "    vocab_size = len(word_vecs)\n",
    "    word_idx_map = dict()\n",
    "    W = np.zeros(shape=(vocab_size + 1, k), dtype='float32')\n",
    "    W[0] = np.zeros(k, dtype='float32')\n",
    "    i = 1\n",
    "    for word in word_vecs:\n",
    "        W[i] = word_vecs[word]\n",
    "        word_idx_map[word] = i\n",
    "        i += 1\n",
    "    return W, word_idx_map\n",
    "\n",
    "\n",
    "def load_bin_vec(fname, vocab):\n",
    "    \"\"\"\n",
    "    Loads 300x1 word vecs from Google (Mikolov) word2vec\n",
    "    \"\"\"\n",
    "    word_vecs = {}\n",
    "    with open(fname, \"rb\") as f:\n",
    "        header = f.readline()\n",
    "        # ~ print header\n",
    "        vocab_size, layer1_size = map(int, header.split())\n",
    "        binary_len = np.dtype('float32').itemsize * layer1_size\n",
    "        # print(vocab_size)\n",
    "        for line in range(vocab_size):\n",
    "            # print(line)\n",
    "            word = []\n",
    "            while True:\n",
    "                ch = f.read(1)\n",
    "                if ch == ' ':\n",
    "                    word = ''.join(word)\n",
    "                    break\n",
    "                if ch != '\\n':\n",
    "                    word.append(ch)\n",
    "            # print(word)\n",
    "            if word in vocab:\n",
    "                # print(word)\n",
    "                word_vecs[word] = np.frombuffer(f.read(binary_len), dtype='float32')\n",
    "            else:\n",
    "                f.read(binary_len)\n",
    "\n",
    "    return word_vecs\n",
    "\n",
    "def add_unknown_words(word_vecs, vocab, min_df=1, k=300):\n",
    "    \"\"\"\n",
    "    For words that occur in at least min_df documents, create a separate word vector.    \n",
    "    0.25 is chosen so the unknown vectors have (approximately) same variance as pre-trained ones\n",
    "    \"\"\"\n",
    "    for word in vocab:\n",
    "        if word not in word_vecs and vocab[word] >= min_df:\n",
    "            word_vecs[word] = np.random.uniform(-0.25, 0.25, k)\n",
    "            \n",
    "w2v_file = \"/projekte/slu/share/GoogleNews-vectors-negative300.bin\"\n",
    "w2v = load_bin_vec(w2v_file, word_to_id)\n",
    "print(\"num words found:\"+ str(len(w2v)))\n",
    "add_unknown_words(w2v, word_to_id, k=300)\n",
    "W, word_idx_map = get_W(w2v, k=300)\n",
    "\n",
    "\n",
    "# Padding the input data\n",
    "from keras.preprocessing import sequence\n",
    "input_length = 350 # average length \n",
    "\n",
    "X_train = sequence.pad_sequences(x_train_org, maxlen=input_length, padding='post', truncating='post')\n",
    "X_dev = sequence.pad_sequences(x_test_org, maxlen=input_length, padding='post', truncating='post')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 350) (25000, 350)\n",
      "350\n",
      "(2003, 300)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "X_train, y_train, X_dev, y_dev, max_len, W  = pickle.load(open('data.p', 'rb'), encoding=\"bytes\")     \n",
    "vocab_size, embedding_size = W.shape\n",
    "\n",
    "print(X_train.shape, X_dev.shape)\n",
    "print(max_len)\n",
    "print(W.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pad(inputs, max_len):\n",
    "    \"\"\"\n",
    "    :param inputs: list of list of tokens\n",
    "    :param max_len: length to which all inputs will be padded\n",
    "    :return: padded inputs, original lenght of unpadded inputs\n",
    "    \"\"\"\n",
    "    lengths = [len(x) for x in inputs]\n",
    "\n",
    "    # we do not want to shorten the sentences\n",
    "    assert max_len >= max(lengths)\n",
    "\n",
    "    for input in inputs:\n",
    "        for i in range(0, max_len - len(input)):\n",
    "            input.append(voc['PAD'])\n",
    "    return inputs, lengths\n",
    "\n",
    "def get_minibatches(inputs, targets, batch_size, max_len, shuffle=False):\n",
    "    \"\"\"\n",
    "    yields padded mini batches (lists of examples)\n",
    "    :param inputs:\n",
    "    :param targets:\n",
    "    :param batch_size:\n",
    "    :param max_len:\n",
    "    :param shuffle: whether to randomize the order of the input-target pairs (important for training)\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    assert len(inputs) == len(targets)\n",
    "    examples = list(zip(inputs, targets))\n",
    "\n",
    "    if shuffle:\n",
    "        random.shuffle(examples)\n",
    "\n",
    "    # take steps of size batch_size, take at least one step\n",
    "    for start_idx in range(0, max(batch_size, len(inputs) - batch_size + 1), batch_size):\n",
    "        batch_examples = examples[start_idx:start_idx + batch_size]\n",
    "        batch_inputs, batch_targets = zip(*batch_examples)\n",
    "\n",
    "        # pad the inputs\n",
    "        batch_inputs, batch_lengths = pad(batch_inputs, max_len)\n",
    "\n",
    "        yield list(batch_inputs), list(batch_lengths), list(batch_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Training\n",
    "To train the model, we need a loss function, here we take the cross entropy loss.\n",
    "We use the Adam optimizer to update the parameters.\n",
    "\n",
    "The train function loops over the training dataset for a provided number of training epochs.\n",
    "One epoch corresponds to running the model once on each training example.\n",
    "\n",
    "Each training step consists of running the forward function of the model, then the backward function and then taking an optimization step.\n",
    "Note that we run the model on _batches_ of training examples, so each training step contains the information of multiple training examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_correct_predictions(outputs, labels):\n",
    "    \"\"\"\n",
    "    :param outputs: predicted probability distribution over labels from model\n",
    "    :param labels: annotation\n",
    "    :return: number of examples in batch for which label was correctly predicted\n",
    "    \"\"\"\n",
    "    # predicted label = label with highest probability\n",
    "    # topk(int) yields tuple: values, indices\n",
    "    pred = nn.functional.softmax(outputs, dim=1).topk(1)[1].squeeze()\n",
    "\n",
    "    correct = pred.eq(labels)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        correct = correct.cpu()\n",
    "\n",
    "    num_correct = torch.sum(correct, dim=0)\n",
    "    return num_correct.numpy()\n",
    "\n",
    "def train(model, X_train, y_train, learning_rate, num_epochs, batch_size, sent_len):\n",
    "    \"\"\"\n",
    "    train model on given training examples X_train with correct labels y_train for num_epochs\n",
    "    :param model: model of type torch.nn\n",
    "    :param X_train:\n",
    "    :param y_train:\n",
    "    :param learning_rate:\n",
    "    :param num_epochs:\n",
    "    :param batch_size:\n",
    "    :param sent_len: maximum sentence length to pad all inputs in X_train\n",
    "    :return: model with trained parameters\n",
    "    \"\"\"\n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss() # contains softmax layer and cross entropy loss, averages over examples in batch\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # train the model\n",
    "    for epoch in range(num_epochs):\n",
    "        train_correct = 0.0\n",
    "        train_correct_checkpoint = 0.0\n",
    "        train_loss = 0.0\n",
    "        for i, (inputs, _, labels) in enumerate(get_minibatches(X_train, y_train, batch_size, sent_len, shuffle=True)):\n",
    "            \n",
    "            # now you also have to transfer the inputs on the GPU\n",
    "            if torch.cuda.is_available():\n",
    "                inputs = Variable(torch.cuda.LongTensor(inputs))\n",
    "                labels = Variable(torch.cuda.LongTensor(labels))\n",
    "            else:\n",
    "                inputs = Variable(torch.LongTensor(inputs))\n",
    "                labels = Variable(torch.LongTensor(labels))\n",
    "\n",
    "            # Forward\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            train_correct += count_correct_predictions(outputs, labels)\n",
    "            \n",
    "            # Backward\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "\n",
    "            # Optimize model parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.data\n",
    "\n",
    "        print('Epoch [%d/%d], Train accuracy: %.2f, Train loss: %.2f'\n",
    "                       % (epoch + 1, num_epochs, train_correct*1.0/len(X_train), train_loss/(len(X_train)/batch_size)))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialize model paramters\n",
      "embedding.weight\n",
      "layer.0.weight\n",
      "layer.0.bias\n",
      "fc.weight\n",
      "fc.bias\n",
      "Epoch [1/5], Train accuracy: 0.82, Train loss: 0.39\n",
      "Epoch [2/5], Train accuracy: 0.89, Train loss: 0.28\n",
      "Epoch [3/5], Train accuracy: 0.91, Train loss: 0.22\n",
      "Epoch [4/5], Train accuracy: 0.94, Train loss: 0.14\n",
      "Epoch [5/5], Train accuracy: 0.98, Train loss: 0.07\n"
     ]
    }
   ],
   "source": [
    "import os  \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\"\n",
    "\n",
    "# model configuration\n",
    "#emb_size = 50\n",
    "num_filters = 100\n",
    "# extract features from tri-grams\n",
    "region_size = 3\n",
    "num_classes = 2\n",
    "\n",
    "# build model\n",
    "net = CNN(vocab_size, embedding_size, max_len, num_classes, num_filters, region_size, W)\n",
    "\n",
    "# place all tensors in model on GPU\n",
    "if torch.cuda.is_available():\n",
    "    net = net.cuda()\n",
    "    \n",
    "\n",
    "\n",
    "# initialize model parameters\n",
    "print(\"initialize model paramters\")\n",
    "for (name, param) in net.named_parameters():\n",
    "    print(name)\n",
    "    nn.init.normal_(param, std=0.01)\n",
    "\n",
    "# training configuration\n",
    "num_epochs = 5\n",
    "learning_rate = 0.001\n",
    "batch_size = 32\n",
    "trained_net = train(net, X_train, y_train, learning_rate, num_epochs, batch_size, max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Evaluation\n",
    "When evaluating the model on the development set, we only run the forward pass and compute the loss and accuracy of the model.\n",
    "We do not run the backward pass and do not optimize the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users0/ortegamo/.local/lib/python3.7/site-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.85, Loss: 0.53\n"
     ]
    }
   ],
   "source": [
    "def evaluate(model, X, y, sent_len):\n",
    "    # loss, do not average over batches for dev set but only do final average over all examples\n",
    "    criterion = nn.CrossEntropyLoss(size_average=False)\n",
    "\n",
    "    # evaluate\n",
    "    dev_loss = 0.0\n",
    "    dev_correct = 0.0\n",
    "    # we do not need to shuffle the dataset\n",
    "    for i, (inputs, _, labels) in enumerate(get_minibatches(X_dev, y_dev, batch_size, sent_len, shuffle=False)):\n",
    "        inputs = Variable(torch.LongTensor(inputs))\n",
    "        labels = Variable(torch.LongTensor(labels))\n",
    "        if torch.cuda.is_available():\n",
    "            inputs = inputs.cuda()\n",
    "            labels = labels.cuda()\n",
    "            \n",
    "        outputs = model(inputs)\n",
    "\n",
    "        dev_loss += criterion(outputs, labels).data\n",
    "\n",
    "        dev_correct += count_correct_predictions(outputs, labels)\n",
    "    \n",
    "    accuracy = dev_correct/len(X_dev)\n",
    "    avg_loss = dev_loss / len(X_dev)\n",
    "    return accuracy, avg_loss\n",
    "\n",
    "# evaluate model on development set after training\n",
    "accuracy, avg_loss = evaluate(trained_net, X_dev, y_dev, max_len)\n",
    "\n",
    "print('Accuracy: %.2f, Loss: %.2f' %(accuracy, avg_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation on the development set can also be included into the training loop to monitor the performance of the model on the development set.\n",
    "This is useful for early stopping and to determine overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialize model paramters\n",
      "embedding.weight\n",
      "layer.0.weight\n",
      "layer.0.bias\n",
      "fc.weight\n",
      "fc.bias\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.7/site-packages/ipykernel_launcher.py:66: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
      "/home/users0/ortegamo/.local/lib/python3.7/site-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Train accuracy: 0.81, Train loss: 0.39, Dev accuracy: 0.87, Dev loss: 0.31\n",
      "Epoch [2/5], Train accuracy: 0.89, Train loss: 0.28, Dev accuracy: 0.86, Dev loss: 0.33\n",
      "Epoch [3/5], Train accuracy: 0.91, Train loss: 0.22, Dev accuracy: 0.86, Dev loss: 0.35\n",
      "Epoch [4/5], Train accuracy: 0.95, Train loss: 0.14, Dev accuracy: 0.85, Dev loss: 0.45\n",
      "Epoch [5/5], Train accuracy: 0.98, Train loss: 0.06, Dev accuracy: 0.85, Dev loss: 0.55\n"
     ]
    }
   ],
   "source": [
    "def train_and_evaluate(model, X_train, y_train, X_dev, y_dev, learning_rate, num_epochs, batch_size, sent_len):\n",
    "    \"\"\"\n",
    "    train model on given training examples X_train with correct labels y_train for num_epochs\n",
    "    evaluate on development set after each epoch\n",
    "    :param model: model of type torch.nn\n",
    "    :param X_train:\n",
    "    :param y_train:\n",
    "    :param X_dev:\n",
    "    :param y_dev:\n",
    "    :param learning_rate:\n",
    "    :param num_epochs:\n",
    "    :param batch_size:\n",
    "    :param sent_len: maximum sentence length to pad all inputs in X_train\n",
    "    :return: model with trained parameters\n",
    "    \"\"\"\n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss() # contains softmax layer and cross entropy loss, averages over examples in batch\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # train the model\n",
    "    for epoch in range(num_epochs):\n",
    "        train_correct = 0.0\n",
    "        train_correct_checkpoint = 0.0\n",
    "        train_loss = 0.0\n",
    "        for i, (inputs, _, labels) in enumerate(get_minibatches(X_train, y_train, batch_size, sent_len, shuffle=True)):\n",
    "            inputs = Variable(torch.LongTensor(inputs))\n",
    "            labels = Variable(torch.LongTensor(labels))\n",
    "            \n",
    "            if torch.cuda.is_available():\n",
    "                inputs = inputs.cuda()\n",
    "                labels = labels.cuda()\n",
    "\n",
    "            # Forward\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            train_correct += count_correct_predictions(outputs, labels)\n",
    "            \n",
    "            # Backward\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "\n",
    "            # Optimize model parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.data\n",
    "        \n",
    "        train_accuracy = train_correct/len(X_train)\n",
    "        train_loss = train_loss/(len(X_train)/batch_size)\n",
    "        dev_accuracy, dev_loss = evaluate(model, X_dev, y_dev, sent_len)\n",
    "\n",
    "        print('Epoch [%d/%d], Train accuracy: %.2f, Train loss: %.2f, Dev accuracy: %.2f, Dev loss: %.2f'\n",
    "                       % (epoch + 1, num_epochs, train_accuracy, train_loss, dev_accuracy, dev_loss))\n",
    "\n",
    "    return model\n",
    "\n",
    "# build model\n",
    "net = CNN(vocab_size, embedding_size, max_len, num_classes, num_filters, region_size, W)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "     net = net.cuda()\n",
    "\n",
    "# initialize model parameters\n",
    "print(\"initialize model paramters\")\n",
    "for (name, param) in net.named_parameters():\n",
    "    print(name)\n",
    "    nn.init.normal(param, std=0.01)\n",
    "    \n",
    "trained_net = train_and_evaluate(net, X_train, y_train, X_dev, y_dev, learning_rate, num_epochs, batch_size, max_len)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
