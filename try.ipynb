{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opening C:/Users/Vishnudatha/Documents/Masters/Sem_4/Deep_Learning/project/keras/data_prosody.train\n",
      "opening C:/Users/Vishnudatha/Documents/Masters/Sem_4/Deep_Learning/project/keras/data_logMel.train\n",
      "opening C:/Users/Vishnudatha/Documents/Masters/Sem_4/Deep_Learning/project/keras/data_prosody.valid\n",
      "opening C:/Users/Vishnudatha/Documents/Masters/Sem_4/Deep_Learning/project/keras/data_logMel.valid\n",
      "opening C:/Users/Vishnudatha/Documents/Masters/Sem_4/Deep_Learning/project/keras/data_prosody.test\n",
      "opening C:/Users/Vishnudatha/Documents/Masters/Sem_4/Deep_Learning/project/keras/data_logMel.test\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "import numpy\n",
    "import pickle\n",
    "\n",
    "directory = \"C:/Users/Vishnudatha/Documents/Masters/Sem_4/Deep_Learning/project/keras/\"\n",
    "\n",
    "names = ['prosody.train', 'logMel.train', 'prosody.valid', 'logMel.valid', 'prosody.test', 'logMel.test']\n",
    "file_paths = [directory + \"data_prosody.train\", directory + \"data_logMel.train\", directory + \"data_prosody.valid\", directory + \"data_logMel.valid\", directory + \"data_prosody.test\", directory + \"data_logMel.test\"]\n",
    "\n",
    "#file_paths = [\"data_prosody.train\", \"data_logMel.train\", \"data_prosody.valid\", \"data_logMel.valid\", \"data_prosody.test\", \"data_logMel.test\"]\n",
    "\n",
    "\n",
    "for file_path, name in zip(file_paths, names):\n",
    "    print('opening ' + file_path)\n",
    "    data = xr.open_dataset(file_path)\n",
    "\n",
    "    # The dataset looks like this:\n",
    "    #\n",
    "    # <xarray.Dataset>\n",
    "    # Dimensions:         (feature_count: 7, instance: 10039, time: 750)\n",
    "    # Dimensions without coordinates: feature_count, instance, time\n",
    "    # Data variables:\n",
    "    #     file_name       (instance) object ...\n",
    "    #     feature_names   (feature_count) object ...\n",
    "    #     feature_value   (instance, time, feature_count) float32 ...\n",
    "    #     speaker_gender  (instance) object ...\n",
    "    #     speech_type     (instance) object ...\n",
    "    #     cv_fold         (instance) int64 ...\n",
    "    #     label_nominal   (instance) object ...\n",
    "    #     label_numeric   (instance) int64 ...\n",
    "    #     arousal         (instance) float32 ...\n",
    "    #     valence         (instance) float32 ...\n",
    "    # Attributes:\n",
    "    #     description:  features and labels of the IEMOCAP dataset\n",
    "\n",
    "    if \".train\" in name or \".valid\" in name:\n",
    "        indices_one_hot_angry = (data['label_nominal'].values == 'anger')\n",
    "        angry_features = data['feature_value'].values[indices_one_hot_angry]\n",
    "\n",
    "        indices_one_hot_happy1 = (data['label_nominal'].values == 'happiness')\n",
    "        indices_one_hot_happy2 = (data['label_nominal'].values == 'excitement')\n",
    "        indices_one_hot_happy = numpy.logical_or(indices_one_hot_happy1, indices_one_hot_happy2)\n",
    "        happy_features = data['feature_value'].values[indices_one_hot_happy]\n",
    "\n",
    "        indices_one_hot_sad = (data['label_nominal'].values == 'sadness')\n",
    "        sad_features = data['feature_value'].values[indices_one_hot_sad]\n",
    "\n",
    "        indices_one_hot_neutral = (data['label_nominal'].values == 'neutral')\n",
    "        neutral_features = data['feature_value'].values[indices_one_hot_neutral]\n",
    "\n",
    "        features = numpy.concatenate([angry_features, happy_features, sad_features, neutral_features])\n",
    "        labels = ['angry' for f in range(len(angry_features))] + ['happy' for f in range(len(happy_features))] + ['sad' for f in range(len(sad_features))] + ['neutral' for f in range(len(neutral_features))]\n",
    "\n",
    "    else:\n",
    "        indices = (data['label_nominal'].values == 'X')\n",
    "        features = data['feature_value'].values[indices]\n",
    "        labels = ['X' for f in range(len(features))]\n",
    "\n",
    "    with open(\"1data_\" + name, 'wb') as f_out:\n",
    "        pickle.dump(features, f_out)\n",
    "        pickle.dump(labels, f_out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_embeddings(path):\n",
    "    with open(path,'r') as f:\n",
    "        words = [line.split(None, 1)[0] for line in f]\n",
    "    with open(path,'r') as f:\n",
    "        word_embeddings = [line.strip().split('\\t', 2)[1] for line in f]\n",
    "    return words,word_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.1  0.2]\n",
      "  [ 0.3  0.4]\n",
      "  [ 0.5  0.6]\n",
      "  [ 0.7  0.8]\n",
      "  [ 0.9  1. ]]]\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "data = array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0])\n",
    "data = data.reshape((1, 5, 2))\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
