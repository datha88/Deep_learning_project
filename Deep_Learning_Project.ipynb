{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Project Topic 1: Dialog Act Classification from Text"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "• Develop a classifier which predicts the dialog acts\n",
    "(e.g. question/statement/backchannel) given an\n",
    "utterance and its context using lexical cues\n",
    "• Example:\n",
    "– “what is that” → question\n",
    "– “anyway it 's the only car that says you know sporty and\n",
    "class uh” → opinion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(fname):\n",
    "    with open(fname,'r') as f:\n",
    "        utt_ID = [line.split(None, 1)[0] for line in f]\n",
    "    with open(fname,'r') as f:\n",
    "        y = [line.split('\\t', 2)[1] for line in f]\n",
    "    with open(fname,'r') as f:\n",
    "        x_train_org =[line.strip().split('\\t', 2)[2] for line in f]\n",
    "        x_train = [x.split(';') for x in x_train_org]\n",
    "    return utt_ID,y,x_train \n",
    "\n",
    "def load_test_data(fname):\n",
    "    with open(fname,'r') as f:\n",
    "        utt_ID = [line.split(None, 1)[0] for line in f]\n",
    "    with open(fname,'r') as f:\n",
    "        x_test_org =[line.strip().split('\\t', 2)[1] for line in f]\n",
    "        x_test = [x.split(';') for x in x_test_org]\n",
    "    return utt_ID,x_test "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load training, validation and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "196502"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utt_ID_train,y_train_org,x_train_org= load_data(\"utterances.train\")\n",
    "x_train_org[0]\n",
    "len(x_train_org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utt_ID_valid,y_valid_org,x_valid_org= load_data(\"utterances.valid\")\n",
    "x_valid_org[0]\n",
    "len(x_valid_org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utt_ID_test,x_test_org= load_test_data(\"utterances.test\")\n",
    "x_test_org[0]\n",
    "len(x_test_org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_input_x_for_length(x):\n",
    "    x_processed=[None]*len(x)\n",
    "    for i in range(len(x)):\n",
    "        res = []\n",
    "        for list in x[i]:\n",
    "            res.append(str(list).lower().split())\n",
    "        x_processed[i]=res\n",
    "    return x_processed\n",
    "\n",
    "def process_input_x(x):\n",
    "    x_processed=[None]*len(x)\n",
    "    for i in range(len(x)):\n",
    "        res = []\n",
    "        for list in x[i]:\n",
    "            res.append(str(list).lower().split())\n",
    "        x_processed[i]=res\n",
    "    x_processed_with_spaces= np.array([[add_spaces(x) for x in uttr] for uttr in x_processed])\n",
    "    return x_processed_with_spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Adjust the input data to average length\n",
    "input_length = 15 # average length \n",
    "\n",
    "x_tr_len= process_input_x_for_length(x_train_org)\n",
    "lengths = []\n",
    "for i in x_tr_len:\n",
    "    for j in i:\n",
    "        lengths.append(len(j))\n",
    "\n",
    "\n",
    "\n",
    "max_length=max(lengths)\n",
    "mean_length=np.int(np.mean(lengths))\n",
    "forty_percentile =np.percentile([np.unique(lengths)],40)\n",
    "print(\"max %d\" % max_length)\n",
    "print(\"mean %d\" % mean_length)\n",
    "\n",
    "print(\"50 percentile %d\" % forty_percentile)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(lengths)\n",
    "plt.ylabel('length')\n",
    "plt.show()\n",
    "\n",
    "def add_spaces(x):\n",
    "    return x[:input_length]+['' for i in range(input_length-len(x))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train,x_valid,x_test\n",
    "x_train=process_input_x(x_train_org)\n",
    "x_valid=process_input_x(x_valid_org)\n",
    "x_test=process_input_x(x_test_org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Printing Input data sizes\")\n",
    "print(\"x train shape\",len(x_train))\n",
    "print(\"x valid shape\",len(x_valid))\n",
    "print(\"x test shape\",len(x_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '',\n",
       "        '', '', ''],\n",
       "       ['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '',\n",
       "        '', '', ''],\n",
       "       ['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '',\n",
       "        '', '', ''],\n",
       "       ['okay', ',', 'uh', '', '', '', '', '', '', '', '', '', '', '', '',\n",
       "        '', '', '', '', '']],\n",
       "      dtype='<U18')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Find uniquewords of x_train,x_valid,x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25493\n"
     ]
    }
   ],
   "source": [
    "uniqueWords = []\n",
    "for i in range(len(x_train_org)):\n",
    "        for j in range(len(x_train[i])):\n",
    "             for k in x_train[i][j]:\n",
    "                    if not k.lower() in uniqueWords:\n",
    "                        uniqueWords.append(k.lower())\n",
    "for i in range(len(x_valid_org)):\n",
    "        for j in range(len(x_valid[i])):\n",
    "             for k in x_valid[i][j]:\n",
    "                    if not k.lower() in uniqueWords:\n",
    "                        uniqueWords.append(k.lower())\n",
    "for i in range(len(x_test_org)):\n",
    "        for j in range(len(x_test[i])):\n",
    "             for k in x_test[i][j]:\n",
    "                    if not k.lower() in uniqueWords:\n",
    "                        uniqueWords.append(k.lower())\n",
    "print(len(uniqueWords))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Word embeddings from Google (Mikolov) word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_W(word_vecs, k=300):\n",
    "    \"\"\"\n",
    "    Get word matrix. W[i] is the vector for word indexed by i\n",
    "    \"\"\"\n",
    "    vocab_size = len(word_vecs)\n",
    "    word_idx_map = dict()\n",
    "    W = np.zeros(shape=(vocab_size + 1, k), dtype='float32')\n",
    "    W[0] = np.zeros(k, dtype='float32')\n",
    "    i = 1\n",
    "    for word in word_vecs:\n",
    "        W[i] = word_vecs[word]\n",
    "        word_idx_map[word] = i\n",
    "        i += 1\n",
    "    return W, word_idx_map\n",
    "\n",
    "\n",
    "def load_bin_vec(fname, vocab):\n",
    "    \"\"\"\n",
    "    Loads 300x1 word vecs from Google (Mikolov) word2vec\n",
    "    \"\"\"\n",
    "    word_vecs = {}\n",
    "    with open(fname, \"rb\") as f:\n",
    "        header = f.readline()\n",
    "        # ~ print(header)\n",
    "        vocab_size, layer1_size = map(int, header.split())\n",
    "        binary_len = np.dtype('float32').itemsize * layer1_size\n",
    "        # print(vocab_size)\n",
    "        for line in range(vocab_size):\n",
    "            # print(line)\n",
    "            word = []\n",
    "            while True:\n",
    "                ch = f.read(1).decode('iso-8859-1')\n",
    "                if ch == ' ':\n",
    "                    word = ''.join(word)\n",
    "                    break\n",
    "                if ch != '\\n':\n",
    "                    word.append(ch)\n",
    "            # print(word)\n",
    "            if word in vocab:\n",
    "                # print(word)\n",
    "                word_vecs[word] = np.frombuffer(f.read(binary_len), dtype='float32')\n",
    "            else:\n",
    "                f.read(binary_len)\n",
    "\n",
    "    return word_vecs\n",
    "\n",
    "def add_unknown_words(word_vecs, vocab, k=300):\n",
    "    \"\"\"\n",
    "    For words that occur in at least min_df documents, create a separate word vector.    \n",
    "    0.25 is chosen so the unknown vectors have (approximately) same variance as pre-trained ones\n",
    "    \"\"\"\n",
    "    for word in vocab:\n",
    "        if word not in word_vecs:\n",
    "            word_vecs[word] = np.random.uniform(-0.25, 0.25, k)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num words found: 21199\n",
      "W shape: (25494, 300)\n",
      "2382 seconds to get the embeddings\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "w2v_file = \"GoogleNews-vectors-negative300.bin\"\n",
    "w2v = load_bin_vec(w2v_file, uniqueWords)\n",
    "print(\"num words found: %d\" % len(w2v))\n",
    "add_unknown_words(w2v, uniqueWords, k=300)\n",
    "W, word_idx_map = get_W(w2v, k=300)\n",
    "\n",
    "print(\"W shape: %s\" % str(W.shape))\n",
    "\n",
    "print(\"%d seconds to get the embeddings\" % (time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_x(x):\n",
    "    l=[]\n",
    "    for i in x:\n",
    "        m=[]\n",
    "        for j in i:\n",
    "            n=[]\n",
    "            for k in j:\n",
    "                n.append(W[word_idx_map[k]])\n",
    "            m.append(n)\n",
    "        l.append(m)\n",
    "    x_em=np.array(l)\n",
    "    l=[]\n",
    "    for i in x_em:\n",
    "        m=[]\n",
    "        for j in i:\n",
    "            m.append(j.flatten())\n",
    "        l.append(m)\n",
    "    x_format=np.array(l)\n",
    "    return x_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Embedd x train, x valid and x test \n",
    "x_em_train= format_x(x_train)\n",
    "print(\"X train embedded\",x_em_train.shape)\n",
    "x_em_valid= format_x(x_valid)\n",
    "print(\"X valid embedded\",x_em_valid.shape)\n",
    "x_em_test= format_x(x_test)\n",
    "print(\"X test embedded\",x_em_test.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Process y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Labels\n",
    "tags = ['%', '%--', '2', 'aa', 'aap', 'ar', \n",
    "          'b', 'ba', 'bc', 'bd', 'bh', 'bk', \n",
    "          'br', 'bs', 'cc', 'co', 'd', 'fa', \n",
    "          'ft', 'g', 'h', 'no', 'qh', 'qo', \n",
    "          'qrr', 'qw', 'qy', 's', 't1', 't3','x']\n",
    "\n",
    "id_to_tags = dict()\n",
    "for id_i,i in enumerate(tags):\n",
    "    id_to_tags[i] = id_i\n",
    "\n",
    "def tag_to_id(y):\n",
    "    y_id=[None]*len(y)\n",
    "    for id_i, i in enumerate(y):\n",
    "        y_id[id_i]=id_to_tags[i]\n",
    "    return np.array(y_id)\n",
    "\n",
    "def id_to_tag(y_tag):\n",
    "    y_id=[None]*len(y_tag)\n",
    "    for id_i, i in enumerate(y_tag):\n",
    "        y_id[id_i]=tags[i]\n",
    "    return np.array(y_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#id_to_tags\n",
    "y_train = tag_to_id(y_train_org)\n",
    "y_valid = tag_to_id(y_valid_org)\n",
    "\n",
    "print(\"y train shape:\",y_train.shape)\n",
    "print(\"y valid shape:\",y_valid.shape)\n",
    "\n",
    "#one hot representation of y_train, y_valid\n",
    "from keras.utils import np_utils\n",
    "\n",
    "y_train_id = np_utils.to_categorical(y_train)\n",
    "y_valid_id = np_utils.to_categorical(y_valid)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes= y_train_id.shape[1]\n",
    "\n",
    "print(\"Num of classes\", num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import LSTM,Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Bidirectional\n",
    "\n",
    "\n",
    "\n",
    "model=Sequential()\n",
    "time_steps=x_em_train.shape[1]\n",
    "features=x_em_train.shape[2]\n",
    "\n",
    "model.add(Bidirectional(LSTM(units= 100,  activation='tanh'),input_shape=(time_steps,features)))\n",
    "\n",
    "model.add(Dense(num_classes, activation='relu'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_em_train, y_train_id, validation_data=(x_em_valid, y_valid_id), epochs=5, batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('my_model.h5')\n",
    "\n",
    "model_eval = model.evaluate(x_em_valid, y_valid_id, batch_size=128, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (model_eval[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_id = model.predict(x_em_test).argmax(axis=1)\n",
    "print(y_test_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = id_to_tag(y_test_id)\n",
    "data = np.column_stack((utt_ID_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"3250569_Kanjur_topic1_result.txt\", data, delimiter='\\t', fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
